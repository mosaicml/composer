train_dataset:
  lm:
    split: train
    datadir:
      - /mnt/gcp/datasets/wikipedia_saved_bert_128
      - /mnt/gcp/datasets/bookcorpus_saved_bert_128
    tokenizer_name: bert-base-uncased
    seed: 19
    shuffle: true 
    drop_last: true 
    subsample_ratio: 1.0 
    use_masked_lm: true
    mlm_probability: 0.15
    train_sequence_length: 128 
    val_sequence_length: 128 
val_dataset:
  lm:
    split: validation
    datadir:
      - /mnt/gcp/datasets/wikipedia_saved_bert_128
      - /mnt/gcp/datasets/bookcorpus_saved_bert_128
    tokenizer_name: bert-base-uncased
    seed: 19
    shuffle: true 
    drop_last: true 
    subsample_ratio: 0.02
    use_masked_lm: true
    mlm_probability: 0.15
    train_sequence_length: 128 
    val_sequence_length: 128 
model:
  bert:
    use_pretrained: false
    tokenizer_name: bert-base-uncased
    pretrained_model_name: bert-base-uncased
optimizer:
  decoupled_adamw:
    lr: 5.0e-4
    betas:
      - 0.9
      - 0.98
    eps: 1.0e-06
    weight_decay: 1.0e-5
schedulers:
  - warmup:
      warmup_method: linear
      warmup_iters: 0.06dur
      warmup_factor: 0
      interval: step
  - linear_decay:
      start_factor: 1.0
      end_factor: 0.0
      total_iters: 0.94dur
      interval: step
      verbose: false
loggers:
  - tqdm: {}
max_duration: 7ep  # Baseline is 256M samples, 7 epochs is ~280M samples
train_batch_size: 4000
eval_batch_size: 2000
seed: 19
device:
  gpu: {}
dataloader:
  pin_memory: true
  persistent_workers: true
  num_workers: 8
  timeout: 0
  prefetch_factor: 2
grad_accum: 2
precision: amp
grad_clip_norm: None
validate_every_n_batches: 1000
validate_every_n_epochs: 1
save_checkpoint:
  folder: bert_checkpoints
  interval: 1
  interval_unit: ep
