train_dataset:
  glue:
    task: cola
    tokenizer_name: bert-base-uncased
    split: train
    max_seq_length: 256
    shuffle: false
    drop_last: false
evaluators:
  - label: glue_cola
    eval_dataset:
      glue:
        task: cola
        tokenizer_name: bert-base-uncased
        split: validation
        max_seq_length: 256
        shuffle: false
        drop_last: false
    metric_names:
      - MatthewsCorrCoef
model:
  bert_classification:
    num_labels: 2
    use_pretrained: true
    pretrained_model_name: bert-base-uncased
optimizers:
  decoupled_adamw:
    lr: 5.0e-5
    betas:
      - 0.9
      - 0.98
    eps: 1.0e-06
    weight_decay: 5.0e-6
schedulers:
  - linear_decay_with_warmup:
      t_warmup: 0.06dur
max_duration: 10ep
train_batch_size: 32
eval_batch_size: 32
seed: 19
dataloader:
  pin_memory: true
  persistent_workers: true
  num_workers: 8
  timeout: 0
  prefetch_factor: 2
grad_accum: 1
eval_interval: 250ba
callbacks:
  - lr_monitor: {}
load_path: https://storage.googleapis.com/llm_checkpoints/bert_checkpoint/bert_checkpoints/ep7.pt
load_weights_only: true
load_strict_model_weights: false
