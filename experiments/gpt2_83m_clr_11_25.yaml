---
# This is for a 8x3080 instance on COTA
image: mosaicml/research:latest
parameters:
  device:
    - gpu
  _n_gpus:
    - 8
  instance_types:
    - cota-g8-3080
  grad_accum:
    - 64
  precision:
    - amp
  callbacks: # List of: profiler, speed_monitor, lr_monitor, classifier_metrics
    - speed_monitor lr_monitor
  
  # can't use {{parameters.schedulers.cosine_warmrestart.whatever*}} in jinja
  # templates since {{parameters.schedulers}} is a string; so instead make
  # up an intermediate variable that we *can* put in our run names
  #__T_0:
  __T_0: 
    - "10ep"

command: >-
  git clone git@github.com:jacobfulano/composer

  cd composer

  git checkout clr_runs

  pip install -e .
  pip install wandb
  pip install --upgrade yahp
  
  composer -n {{ parameters._n_gpus }} examples/run_mosaic_trainer.py
  -f composer/yamls/models/gpt2_83m_clr.yaml 
  --loggers wandb 
  --device.gpus.n_gpus {{ parameters._n_gpus }}
  --dataloader.num_workers 15
  --train_dataset.lm.datadir /localdisk/openwebtext_saved
  --val_dataset.lm.datadir /localdisk/openwebtext_saved
  --loggers.wandb.project gpt2_clr_sweep 
  --loggers.wandb.name cwr_T0_{{parameters.__T_0}}
  --schedulers.cosine_warmrestart.eta_min 0.001 
  --callbacks.speed_monitor.window_size 10
  --schedulers.cosine_warmrestart.T_0 {{parameters.__T_0}}