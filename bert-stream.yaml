train_dataset:
  streaming_lm:
    dataset_name: c4
    dataset_config_name: en
    split: train
    max_samples: 275184000
    max_seq_len: 128
    group_method: truncate  # TODO: check that the code for this one is right
    tokenizer_name: bert-base-uncased
    use_masked_lm: true  # TODO: check that the code for this one is right
    mlm_probability: 0.15
    seed: 17
    shuffle: true
    drop_last: true
evaluators:
  evaluator:
    label: bert_pre_training
    eval_dataset:
      streaming_lm:
        dataset_name: c4
        dataset_config_name: en
        split: validation
        max_samples: 32000
        max_seq_len: 128
        group_method: truncate
        tokenizer_name: bert-base-uncased
        use_masked_lm: true
        mlm_probability: 0.15
        seed: 17
        shuffle: false
        drop_last: true
    metric_names:
      - LanguageCrossEntropy
      - MaskedAccuracy
model:
  bert:
    use_pretrained: false
    tokenizer_name: bert-base-uncased
    pretrained_model_name: bert-base-uncased
optimizers:
  decoupled_adamw:
    lr: 5.0e-4
    betas:
      - 0.9
      - 0.98
    eps: 1.0e-06
    weight_decay: 1.0e-5
schedulers:
  - linear_decay_with_warmup:
      t_warmup: 0.06dur
      alpha_f: 0.02
loggers:
  - progress_bar: {}
  - wandb: {}
  #- object_store:
  #    object_store_hparams:
  #      provider: GOOGLE_STORAGE
  #      container:   # TODO: the user should fill this in
  #      key_environ: GCS_KEY
  #      secret_environ: GCS_SECRET
callbacks:
  - speed_monitor:
      window_size: 500
  - lr_monitor: {}
max_duration: 1000ba
train_batch_size: 2000
eval_batch_size: 1000
seed: 3407
device:
    gpu: {}
dataloader:
  pin_memory: true
  persistent_workers: true
  num_workers: 1
  timeout: 0
  prefetch_factor: 2
precision: amp
grad_clip_norm: -1.0
eval_interval: 3500ba
#save_folder: bert_checkpoints
save_interval: 3500ba
#save_artifact_name: "{run_name}/checkpoints/ep{epoch}-ba{batch}-rank{rank}-{total_wct}wct"
grad_accum: 1
