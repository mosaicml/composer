{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b755249",
   "metadata": {},
   "source": [
    "# ðŸ¥¡ Exporting for Inference\n",
    "\n",
    "Composer models are also `torch.nn.Module`, and thus can be exported like any other PyTorch module. In this tutorial, we walk through how to export your models into various common formats: [ONNX](https://onnx.ai/), [TorchScript](https://pytorch.org/docs/stable/jit.html), and [torch.fx](https://pytorch.org/docs/stable/fx.html). For more detailed options and configuration settings, please consult the linked documentation.\n",
    "\n",
    "## Algorithm compatibility\n",
    "Some of our algorithms alter the model architecture in ways that may render them incompatible with some of the export procedures above. For example, BlurPool replaces some instances of `Conv2d` with `BlurConv2d` layers which are not compatible with `torch.fx` as they have data-dependant control flow. \n",
    "\n",
    "The following table shows which algorithms are compatible with which export formats for inference.\n",
    "\n",
    "|                        | torchscript | torch.fx | ONNX |\n",
    "|------------------------|-------------|----------|------|\n",
    "| apply_blurpool         | &check;           |          | &check;    |\n",
    "| apply_factorization    |             | &check;        | &check;    |\n",
    "| apply_ghost_batchnorm  | &check;           |          | &check;    |\n",
    "| apply_squeeze_excite   | &check;           | &check;        | &check;    |\n",
    "| apply_stochastic_depth | &check;           | &check;        | &check;    |\n",
    "| apply_channels_last    | &check;           | &check;        | &check;    |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46de5fce",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "First, we install composer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c3e0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install mosaicml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de762c7",
   "metadata": {},
   "source": [
    "## Create the model\n",
    "\n",
    "First, we create the model we'd like to export, which in this case is based on a ResNet-50 from torchvision, but with our SqueezeExcite algorithm applied, which adds SqueezeExcite modules after certain `Conv2d` layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6197713c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet\n",
    "from composer.models import ComposerClassifier\n",
    "import composer.functional as cf\n",
    "\n",
    "model = ComposerClassifier(module=resnet.resnet50())\n",
    "model = cf.apply_squeeze_excite(model)\n",
    "\n",
    "# switch to eval mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4bd162",
   "metadata": {},
   "source": [
    "Printing the model shows the new `Bottleneck` layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b876784",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8517a2d2",
   "metadata": {},
   "source": [
    "## ONNX\n",
    "\n",
    "[ONNX](https://onnx.ai/) is a popular model format that can then be consumed by many third-party tools (e.g. TensorRT, OpenVINO) to optimize the model for specific hardware devices. \n",
    "\n",
    "Note: ONNX does not have a prebuild wheel for Mac M1/M2 chips yet, so is not pip installable. Skip this section if you are running on a Mac laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e496af25",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install onnx\n",
    "%pip install onnxruntime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd2d584",
   "metadata": {},
   "source": [
    "The `ComposerClassifier`'s forward method takes as input a pair of tensors `(input, label)`, so we create a dummy tensor sizes for the ONNX export:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f787ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "input = (torch.rand(4, 3, 112, 112), torch.Tensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041e1c76",
   "metadata": {},
   "source": [
    "Then we are ready to run the export with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98d7d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "torch.onnx.export(\n",
    "    model=model,\n",
    "    args=(input,),\n",
    "    f='model.onnx',\n",
    "    input_names=['input'],\n",
    "    output_names=['output'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b255546",
   "metadata": {},
   "source": [
    "Let's load the model and check that everything was exported properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1438c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "onnx_model = onnx.load('model.onnx')\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8623a917",
   "metadata": {},
   "source": [
    "Lastly, we can run inference with the model and check that the model indeed runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac417341",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "# run inference\n",
    "ort_session = ort.InferenceSession('model.onnx')\n",
    "outputs = ort_session.run(\n",
    "    None,\n",
    "    {'input': input[0].numpy()},\n",
    ")\n",
    "\n",
    "print(f\"The predicted classes are {np.argmax(outputs[0], axis=1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca091f8e",
   "metadata": {},
   "source": [
    "Note: As the model is randomly initialized, and the input tensor is random, the output classes in this example have no meaning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c172d596",
   "metadata": {},
   "source": [
    "## Torchscript\n",
    "\n",
    "Torchscript creates models from PyTorch code that can be saved and also optimized for deployment, and is the tooling is native to pytorch. The below command "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7564a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "input = (torch.rand(4, 3, 112, 112), torch.Tensor())\n",
    "\n",
    "scripted_model = torch.jit.script(model)\n",
    "scripted_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab1e98c",
   "metadata": {},
   "source": [
    "We can then run inference by passing in our input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cf7a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = scripted_model(input)\n",
    "output.shape\n",
    "print(f\"The predicted classes are {torch.argmax(output, dim=1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7eb9ea2",
   "metadata": {},
   "source": [
    "The compiled model can also be saved using `torch.jit`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dddfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.jit.save(scripted_model, 'scripted_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5909d39f",
   "metadata": {},
   "source": [
    "## Torch.fx\n",
    "\n",
    "FX is a recent toolkit to transform pytorch modules that allows for advanced graph manipulation and code generation capabilities. Eventually, pytorch will be adding quantization with FX (e.g. see [FX Graph Mode Quantization](https://pytorch.org/tutorials/prototype/fx_graph_mode_quant_guide.html)) and other optimization procedures. Composer is also starting to add algorithms that use `torch.fx` in for graph optimization, so look forward to more of these in the future!\n",
    "\n",
    "Tracing a model with `torch.fx` is fairly straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0039f4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "traced_model = torch.fx.symbolic_trace(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8788f688",
   "metadata": {},
   "source": [
    "Then, we can see all the nodes in the graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fd3c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "traced_model.graph.print_tabular()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140aebd9",
   "metadata": {},
   "source": [
    "And also run inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ce75a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = traced_model(input)\n",
    "print(f\"The predicted classes are {torch.argmax(output, dim=1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418159b8",
   "metadata": {},
   "source": [
    "`torch.fx` is powerful, but one of the key limitations of this method is that it does not support dynamic control flow (e.g. `if` statements or loop that are data-dependant). Therefore, some algorithms, such as BlurPool, are currently not supported. We have ongoing work to bring `torch.fx` support to all our algorithms."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
