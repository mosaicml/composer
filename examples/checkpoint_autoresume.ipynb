{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ⏯️ Autoresume Training\n",
    "\n",
    "When the Trainer is configured with `autoresume=True`, it will automatically look for existing checkpoints and resume training. If no checkpoints exist, it'll start a new training run. This allows you to automatically resume from any faults, with no code changes.\n",
    "\n",
    "To see this example in action, run this notebook twice.\n",
    "\n",
    "* The first time the notebook is run, the trainer will save a checkpoint to the `save_folder` and train\n",
    "  for one epoch.\n",
    "* Any subsequent time the notebook is run, the trainer will resume from the latest checkpoint. If\n",
    "  the latest checkpoint was saved at ``max_duration``, meaning all training is finished, the Trainer will\n",
    "  exit immediately with an error that no training would occur.\n",
    "\n",
    "To simulate a flaky spot instance, try interrupting the notebook (e.g. Ctrl-C) midway through the\n",
    "first training run (say, after epoch 0 is finished). Notice how the progress bars would resume at the next\n",
    "epoch and not repeat any training already completed.\n",
    "\n",
    "This feature does not require code or configuration changes to distinguish between starting a new training\n",
    "run or automatically resuming from an existing one, making it easy to use Composer on preemptable cloud instances.\n",
    "Simply configure the instance to start Composer with the same command every time until training has finished!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies\n",
    "\n",
    "Install composer, if it isn't already installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install mosaicml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data\n",
    "from torch.optim import SGD\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from composer import Trainer\n",
    "from composer.models.classify_mnist import mnist_model\n",
    "\n",
    "# Configure the trainer -- here, we train a simple MNIST classifier\n",
    "model = mnist_model(num_classes=10)\n",
    "optimizer = SGD(model.parameters(), lr=0.01)\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=MNIST('~/datasets', train=True, download=True, transform=ToTensor()),\n",
    "    batch_size=2048,\n",
    ")\n",
    "eval_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=MNIST('~/datasets', train=True, download=True, transform=ToTensor()),\n",
    "    batch_size=2048,\n",
    ")\n",
    "\n",
    "# When using `autoresume`, it is required to specify the `run_name` is required, so\n",
    "# Composer will know which training run to resume\n",
    "run_name = 'my_autoresume_training_run'\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    max_duration='5ep',\n",
    "    optimizers=optimizer,\n",
    "\n",
    "    # Train Data Configuration\n",
    "    train_dataloader=train_dataloader,\n",
    "    train_subset_num_batches=5,  # For this example, limit each epoch to 5 batches\n",
    "\n",
    "    # Evaluation Configuration\n",
    "    eval_dataloader=eval_dataloader,\n",
    "    eval_subset_num_batches=5,  # For this example, limit evaluation to 5 batches\n",
    "\n",
    "    # Checkpoint Configuration\n",
    "    run_name=run_name,\n",
    "    save_folder='./my_autoresume_training_run',\n",
    "    save_interval='1ep',\n",
    "\n",
    "    # Configure autoresume!\n",
    "    autoresume=True,\n",
    ")\n",
    "\n",
    "print('Training!')\n",
    "\n",
    "# Train!\n",
    "trainer.fit()\n",
    "\n",
    "# Print the number of trained epochs (should always bee the `max_duration`, which is 5ep)\n",
    "print(f'\\nNumber of epochs trained: {trainer.state.timestamp.epoch}')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
