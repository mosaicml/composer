{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Composer BERT fine-tuning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMWJI/NlM8J5QL/VXsPEoxE"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VutAsXaoVvjH"
      },
      "outputs": [],
      "source": [
        "# Install HuggingFace transformers and datasets\n",
        "!pip install transformers datasets\n",
        "\n",
        "# Install composer\n",
        "# !pip install mosaicml\n",
        "# To install from source instead of the last release, use the below command instead:\n",
        "!pip install git+https://github.com/mosaicml/composer.git@dev"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction"
      ],
      "metadata": {
        "id": "iCTYz0gLNPOc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This tutorial will demonstrate how to fine-tune a pretrained HuggingFace transformer using the composer library! Composer provides a highly optimized and functional training loop and the ability to compose several methods that can accelerate training.\n",
        "\n",
        "We will focus on fine-tuning a pretrained BERT-base model on the Stanford Sentiment Treebank v2 (SST-2) dataset. After fine-tuning, the BERT model should be able to determine if a setence has positive or negative sentiment.\n",
        "\n",
        "Let's do this ðŸš€"
      ],
      "metadata": {
        "id": "IompLzNN4pu9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining a Composer Model "
      ],
      "metadata": {
        "id": "v_psvpK4-km9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first task is to create a composer model. A composer model defines four components for the composer trainer:\n",
        "- `forward()` - parses the dataloader output for the model's forward function and extracts the necessary components of the model's output for loss calculation.\n",
        "- `loss()` - computes the loss for the current batch using the model and dataloader outputs.\n",
        "- `validate()` - parses the dataloader and model output for the torchmetrics.\n",
        "- `metrics()` - defines the torchmetrics to use during training/validation."
      ],
      "metadata": {
        "id": "UJehTwuirpQa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from torchmetrics import Accuracy\n",
        "from torchmetrics.collections import MetricCollection\n",
        "from composer.models.base import ComposerModel\n",
        "from composer.models.nlp_metrics import LanguageCrossEntropyLoss\n",
        "\n",
        "# Define a Composer Model\n",
        "class ComposerBERT(ComposerModel):\n",
        "  def __init__(self, model):\n",
        "    super().__init__()\n",
        "    self.module = model\n",
        "\n",
        "    # Metrics\n",
        "    self.train_loss = LanguageCrossEntropyLoss()\n",
        "    self.val_loss = LanguageCrossEntropyLoss()\n",
        "\n",
        "    self.train_acc = Accuracy()\n",
        "    self.val_acc = Accuracy()\n",
        "\n",
        "  def forward(self, batch):\n",
        "    output = self.module(**batch)\n",
        "    return output\n",
        "\n",
        "  def loss(self, outputs, batch):\n",
        "    return outputs['loss']\n",
        "\n",
        "  def validate(self, batch):\n",
        "    labels = batch.pop('labels')\n",
        "    output = self.forward(batch)\n",
        "    output = output['logits']\n",
        "\n",
        "    return (output, labels)\n",
        "  \n",
        "  def metrics(self, train: bool = False):\n",
        "    return MetricCollection([self.train_loss, self.train_acc]) if train else MetricCollection([self.val_loss, self.val_acc])\n",
        "\n",
        "# Create a BERT sequence classification model using HuggingFace transformsers\n",
        "model = transformers.AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2) # in BERT hparams\n",
        "\n",
        "# Package as a composer model\n",
        "composer_model = ComposerBERT(model)"
      ],
      "metadata": {
        "id": "kAsTKWfknAyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating dataloaders"
      ],
      "metadata": {
        "id": "xtfy9TzC6CAX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will download and tokenize the SST-2 datasets. "
      ],
      "metadata": {
        "id": "vpsHvZXfAl9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "from multiprocessing import cpu_count\n",
        "\n",
        "# Create BERT tokenizer\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained('bert-base-uncased') # from transformer_shared\n",
        "def tokenize_function(sample):\n",
        "  return tokenizer(text=sample['sentence'], padding=\"max_length\", max_length=256, truncation=True)\n",
        "\n",
        "# Tokenize SST-2\n",
        "sst2_dataset = datasets.load_dataset(\"glue\", \"sst2\")\n",
        "tokenized_sst2_dataset = sst2_dataset.map(tokenize_function,\n",
        "                                          batched=True, \n",
        "                                          num_proc=cpu_count(),\n",
        "                                          batch_size=1000,\n",
        "                                          remove_columns=['idx', 'sentence'])\n",
        "\n",
        "# Split dataset into train and validation sets\n",
        "train_dataset = tokenized_sst2_dataset[\"train\"]\n",
        "eval_dataset = tokenized_sst2_dataset[\"validation\"]"
      ],
      "metadata": {
        "id": "BneG48b45jyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we will create a PyTorch `Dataloader` for each of the datasets generated in the previous block."
      ],
      "metadata": {
        "id": "aNOXiQjN-QbJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "data_collator = transformers.data.data_collator.default_data_collator\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=False, drop_last=False, collate_fn=data_collator)\n",
        "eval_dataloader = DataLoader(eval_dataset,batch_size=16, shuffle=False, drop_last=False, collate_fn=data_collator)"
      ],
      "metadata": {
        "id": "_WRp1JVIApWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use the composer `Trainer`, we need to define a `split_batch` function. This function defines how to split the dataloader output into several \"microbatches\". Microbatchs are chunks of the batch that were divided based on the amount of gradient accumulation used."
      ],
      "metadata": {
        "id": "O6XEXJIakP73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from composer.core import DataSpec\n",
        "\n",
        "def split_batch_dict(batch, n_microbatches: int):\n",
        "      chunked = {k: v.chunk(n_microbatches) for k, v in batch.items()}\n",
        "      num_chunks = len(list(chunked.values())[0])\n",
        "      return [{k: v[idx] for k, v in chunked.items()} for idx in range(num_chunks)]\n",
        "\n",
        "train_dataspec = DataSpec(dataloader=train_dataloader,\n",
        "                          split_batch=split_batch_dict)\n",
        "eval_dataspec = DataSpec(dataloader=eval_dataloader,\n",
        "                         split_batch=split_batch_dict)\n"
      ],
      "metadata": {
        "id": "pqVuEeEeXChD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimizers and Learning Rate Schedulers"
      ],
      "metadata": {
        "id": "KYtPX335c5Vf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The last setup step is to create an optimizer and a learning rate scheduler. We will use PyTorch's AdamW optimizer and linear learning rate scheduler since these are typically used to fine-tune BERT on tasks such as SST-2."
      ],
      "metadata": {
        "id": "bnhYMAMyaQnU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import LinearLR\n",
        "\n",
        "\n",
        "optimizer = AdamW(params=composer_model.parameters(), lr=3e-5, betas=(0.9, 0.98), eps=1e-6, weight_decay=3e-6)\n",
        "linear_lr_decay = LinearLR(optimizer, start_factor=1.0, end_factor=0, total_iters=150)"
      ],
      "metadata": {
        "id": "iYph4dL2ECAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Composer Trainer"
      ],
      "metadata": {
        "id": "WGOTL_0lyNqr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now specify a composer `Trainer` object and run our training! `Trainer` has many arguments that are described in our [documentation](https://docs.mosaicml.com/en/v0.3.1/trainer.html#composer.Trainer), but let's discuss the less obvious arguments used below:\n",
        "- `max_duration` - a string specifying how long to train either in terms of batches (e.g. '10ba' is 10 batches) or epochs (e.g. '1ep' is 1 epoch).\n",
        "- `schedulers` - a list of PyTorch learning rate schedulers that will be composed together.\n",
        "- `device` - specifies if the training will be done on CPU or GPU by using 'cpu' or 'gpu', respectively.\n",
        "- `train_subset_num_batches` - specifies the number of training batches to use for each epoch. This is not a necessary argument, but is useful for quickly testing code.\n",
        "- `precision` - whether to do the training in full precision 'fp32' or mixed precision 'amp'. Mixed precision provides almost 2x speedup in training time on certain hardware. If you get a P100, try `precision='amp'`!\n",
        "- `seed` - sets the random seed for the training run, so the results are reproducible!\n",
        "\n"
      ],
      "metadata": {
        "id": "ZNMEqf2RyToY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from composer import Trainer\n",
        "\n",
        "# Create Trainer Object\n",
        "trainer = Trainer(model=composer_model, \n",
        "                  train_dataloader=train_dataspec,\n",
        "                  eval_dataloader=eval_dataspec,\n",
        "                  max_duration=\"1ep\",\n",
        "                  optimizers=optimizer,\n",
        "                  schedulers=[linear_lr_decay],\n",
        "                  device='gpu',\n",
        "                  train_subset_num_batches=150,\n",
        "                  precision='fp32',\n",
        "                  seed=17\n",
        "                  )\n",
        "# Start training\n",
        "trainer.fit()"
      ],
      "metadata": {
        "id": "AKuR_y5MCNxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualizing Results"
      ],
      "metadata": {
        "id": "j3irz05T5Jpd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our model reaches almost 89% accuracy with only 100 iterations of training! Let's visualize a few samples from the validation set to see how our model performs."
      ],
      "metadata": {
        "id": "H22H3TFD5SWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "eval_batch = next(iter(eval_dataloader))\n",
        "\n",
        "# Move batch to gpu\n",
        "eval_batch = {k: v.cuda() for k, v in eval_batch.items()}\n",
        "with torch.no_grad():\n",
        "  predictions = composer_model(eval_batch)[\"logits\"].argmax(dim=1)\n",
        "\n",
        "# Visualize only 5 samples\n",
        "predictions = predictions[:6]\n",
        "\n",
        "label = ['negative', 'positive']\n",
        "for i, prediction in enumerate(predictions[:6]):\n",
        "  sentence = sst2_dataset[\"validation\"][i][\"sentence\"]\n",
        "  correct_label = label[sst2_dataset[\"validation\"][i][\"label\"]]\n",
        "  prediction_label = label[prediction]\n",
        "  print(f\"Sample: {sentence}\")\n",
        "  print(f\"Label: {correct_label}\")\n",
        "  print(f\"Prediction: {prediction_label}\")\n",
        "  print()\n"
      ],
      "metadata": {
        "id": "Lh-VE_Gm7lKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion"
      ],
      "metadata": {
        "id": "dvjDXCeqLvc1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This tutorial showed how to use the composer `Trainer` to fine-tune a pre-trained BERT on a subset of the SST-2 dataset. We focused on the composer's basic functionality, but there are many more tools such as easy to use gradient accumulation and multi-GPU training! Check out many other features at our [documentation](https://docs.mosaicml.com/en/latest)."
      ],
      "metadata": {
        "id": "OpeCzTxOLxwx"
      }
    }
  ]
}