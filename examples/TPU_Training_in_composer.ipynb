{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”Œ Training with TPUs\n",
    "\n",
    "Composer provides beta support for single core training on TPUs. We integrate with the `torch_xla` backend, for installation instructions and more details, see: https://github.com/pytorch/xla. \n",
    "\n",
    "In this tutorial, we train a ResNet-20 on CIFAR10 using a single TPU core. The setup is exactly the same as with any other device, except the model must be moved to the device before passing to our `Trainer`. We specify `device=tpu` to enable the trainer to use TPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "__O91tR7Rh7O"
   },
   "source": [
    "As prerequisites, first install `torch_xla` and the latest composer version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wc711rY5RxFy"
   },
   "outputs": [],
   "source": [
    "%pip install cloud-tpu-client==0.10 torch==1.12.0 https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-1.12-cp37-cp37m-linux_x86_64.whl\n",
    "%pip install mosaicml\n",
    "\n",
    "from composer import Trainer\n",
    "from composer import models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AqOqlKMNR3CR"
   },
   "source": [
    "Next, we define the model and optimizer. TPUs require the model to be moved to the device _before_ the optimizer is created, which we do here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0J8AUm3HSAQH"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_xla.core.xla_model as xm\n",
    "\n",
    "model = models.composer_resnet_cifar(model_name='resnet_20', num_classes=10)\n",
    "model = model.to(xm.xla_device())\n",
    "\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=0.02,\n",
    "    momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DqjBwfb8SE0G"
   },
   "source": [
    "Creating the CIFAR10 dataset and dataloaders are exactly the same as with other non-TPU devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YwJEDZWxSJUX"
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "data_directory = \"../data\"\n",
    "\n",
    "# Normalization constants\n",
    "mean = (0.507, 0.487, 0.441)\n",
    "std = (0.267, 0.256, 0.276)\n",
    "\n",
    "batch_size = 1024\n",
    "\n",
    "cifar10_transforms = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean, std)])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(data_directory, train=True, download=True, transform=cifar10_transforms)\n",
    "test_dataset = datasets.CIFAR10(data_directory, train=False, download=True, transform=cifar10_transforms)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jnCqqWAhSafT"
   },
   "source": [
    "Lastly, we train for 20 epochs on the TPU by simply adding `device='tpu'` as an argument to the Trainer. \n",
    "\n",
    "Note: we currently only support single-core TPUs in this beta release. Future release will include multi-core TPU support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TAZJT9LGSiyB"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    device=\"tpu\",\n",
    "    eval_dataloader=test_dataloader,\n",
    "    optimizers=optimizer,\n",
    "    max_duration='20ep',\n",
    "    eval_interval=1,\n",
    ")\n",
    "\n",
    "trainer.fit()"
   ]
  }
 ],
 "metadata": {
  "gpuClass": "standard",
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
