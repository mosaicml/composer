{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# âš¡ Migrating from PTL\n",
    "\n",
    "[PyTorch Lightning](https://www.pytorchlightning.ai/) is a popular and very well designed framework for training deep learning models. If you are interested in trying our efficient algorithms and using the Composer trainer, the below is a quick guide on how to adapt your models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's get started! We'll first install dependencies and define the data and model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Dependencies\n",
    "\n",
    "If you haven't already, let's install Composer and PyTorch Lightning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install mosaicml pytorch-lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll go through the process of migrating the Resnet18 on CIFAR10 model from PTL to Composer. We will be following the PTL example [here](https://pytorch-lightning.readthedocs.io/en/stable/notebooks/lightning_examples/cifar10-baseline.html). \n",
    "\n",
    "First, some relevant imports, as well as creating the model as in the PTL tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models\n",
    "from pytorch_lightning import LightningModule\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "def create_model():\n",
    "    model = torchvision.models.resnet18(pretrained=False, num_classes=10)\n",
    "    model.conv1 = nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "    model.maxpool = nn.Identity()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is standard, we setup the training data for CIFAR10 using `torchvision` datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "\n",
    "transform = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ]\n",
    ")\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(trainset, batch_size=256, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(testset, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PTL Lightning Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the PTL tutorial, we use the `LitResnet` model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.functional import accuracy\n",
    "\n",
    "class LitResnet(LightningModule):\n",
    "    def __init__(self, lr=0.05):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = create_model()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        return F.log_softmax(out, dim=1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def evaluate(self, batch, stage=None):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = accuracy(preds, y)\n",
    "\n",
    "        if stage:\n",
    "            self.log(f\"{stage}_loss\", loss, prog_bar=True)\n",
    "            self.log(f\"{stage}_acc\", acc, prog_bar=True)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self.evaluate(batch, \"val\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        self.evaluate(batch, \"test\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.SGD(\n",
    "            self.model.parameters(),\n",
    "            lr=self.hparams.lr,\n",
    "            momentum=0.9,\n",
    "            weight_decay=5e-4,\n",
    "        )\n",
    "        steps_per_epoch = 45000 // 256\n",
    "        scheduler_dict = {\n",
    "            \"scheduler\": OneCycleLR(\n",
    "                optimizer,\n",
    "                0.1,\n",
    "                epochs=30,\n",
    "                steps_per_epoch=steps_per_epoch,\n",
    "            ),\n",
    "            \"interval\": \"step\",\n",
    "        }\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler_dict}\n",
    "\n",
    "PTLModel = LitResnet(lr=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `LitModel` to Composer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that up to here, we have only used pytorch lightning code. Here we will transfer the PTL module to be compatible with Composer. There are a few major differences:\n",
    "\n",
    "* The `training_step` is broken into two parts, the `forward` and the `loss` methods. This is needed since our algorithms (such as label smoothing or selective backprop) sometimes need to intercept and modify the loss. \n",
    "* Optimizers and schedulers are passed directly to the `Trainer` during initialization.\n",
    "* Our `forward` step accepts as input the entire batch and has to take care of unpacking the batch. \n",
    "\n",
    "For more information about the `ComposerModel` format, see our [guide](https://docs.mosaicml.com/en/stable/composer_model.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.classification.accuracy import Accuracy\n",
    "from composer.models.base import ComposerModel \n",
    "PTLmodel = LitResnet(lr=0.05)\n",
    "\n",
    "class MosaicResnet(ComposerModel):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = create_model()\n",
    "        self.acc = Accuracy()\n",
    "\n",
    "    def loss(self, outputs, batch, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Accepts the outputs from forward() and the batch\n",
    "        \"\"\"\n",
    "        x, y = batch  # unpack the labels\n",
    "        return F.nll_loss(outputs, y)\n",
    "\n",
    "    def metrics(self, train):\n",
    "        return self.acc\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, _ = batch\n",
    "        y = self.model(x)\n",
    "        return F.log_softmax(y, dim=1) \n",
    "    \n",
    "    def validate(self, batch):\n",
    "        _, targets = batch\n",
    "        outputs = self.forward(batch)\n",
    "        return outputs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instantiate the Mosaic trainer similarly by specifying \n",
    "the model, dataloaders, optimizers, and max_duration (epochs). For more details on the trainer arguments, see our [Using the Trainer](https://docs.mosaicml.com/en/stable/trainer/using_the_trainer.html) guide.\n",
    "\n",
    "Now you are ready to insert your algorithms! As an example, here we add the [BlurPool](https://docs.mosaicml.com/en/latest/method_cards/blurpool.html) algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from composer import Trainer\n",
    "from composer.algorithms import BlurPool\n",
    "\n",
    "model = MosaicResnet()\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=0.05,\n",
    "    momentum=0.9,\n",
    "    weight_decay=5e-4,\n",
    ")\n",
    "\n",
    "steps_per_epoch = 45000 // 256\n",
    "\n",
    "scheduler = OneCycleLR(\n",
    "    optimizer,\n",
    "    0.1,\n",
    "    epochs=30,\n",
    "    steps_per_epoch=steps_per_epoch,  \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    algorithms=[\n",
    "        BlurPool(\n",
    "            replace_convs=True,\n",
    "            replace_maxpools=True,\n",
    "            blur_first=True\n",
    "        ),\n",
    "    ],\n",
    "    train_dataloader=train_dataloader,\n",
    "    device=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "    eval_dataloader=test_dataloader,\n",
    "    optimizers=optimizer,\n",
    "    schedulers=scheduler,\n",
    "    step_schedulers_every_batch=True,  # interval should be step                  \n",
    "    max_duration='2ep',\n",
    "    eval_interval=1,\n",
    "    train_subset_num_batches=1,\n",
    ")\n",
    "trainer.fit()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
