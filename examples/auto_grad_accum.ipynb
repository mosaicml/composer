{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ♻️ Auto Grad Accum\n",
    "\n",
    "This notebook will demonstrate how to use automatic gradient accumulation to avoid CUDA OOMs, regardless of your batch size choice, GPU type, and number of devices. Experiment with different combinations and see how it works!\n",
    "\n",
    "For details of the implementation, see our [Auto Grad Accum](https://docs.mosaicml.com/en/latest/notes/auto_grad_accum.html) documentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by installing composer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install mosaicml "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Our Workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first set our logging level to `debug` so we can see the trainer reporting the dynamic gradient accumulation changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the CIFAR10 dataset with a ResNet56 model, and some standard optimization settings. For the purposes of this notebook, we'll choose very large batch size, and also increase the image size to 96x 96, such that you would typically hit CUDA Out-of-Memory errors on most GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import composer\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "torch.manual_seed(42) # For replicability\n",
    "\n",
    "data_directory = \"./data\"\n",
    "\n",
    "# Normalization constants\n",
    "mean = (0.507, 0.487, 0.441)\n",
    "std = (0.267, 0.256, 0.276)\n",
    "\n",
    "# choose a very large batch size\n",
    "batch_size = 2048\n",
    "\n",
    "cifar10_transforms = transforms.Compose([\n",
    "  transforms.ToTensor(), \n",
    "  transforms.Normalize(mean, std),\n",
    "  transforms.Resize(size=[96, 96])  # choose a large image size\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(data_directory, train=True, download=True, transform=cifar10_transforms)\n",
    "test_dataset = datasets.CIFAR10(data_directory, train=False, download=True, transform=cifar10_transforms)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from composer import models\n",
    "model = models.ComposerResNetCIFAR(model_name='resnet_56', num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run our trainer code with `grad_accum=='auto'`. setting. Note that this demo requires a GPU to demonstrate automatic gradient accumulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.cuda.is_available(), \"Demonstrating automatic gradient accumulation requires a GPU.\"\n",
    "\n",
    "trainer = composer.trainer.Trainer(\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    eval_dataloader=test_dataloader,\n",
    "    max_duration=\"1ep\",\n",
    "    grad_accum='auto',\n",
    "    device='gpu'\n",
    ")\n",
    "\n",
    "\n",
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on your GPU type, you should see some logs that increase the gradient accumulation dynamically until the model fits into memory, prior to the start of training, e.g. something like:\n",
    "\n",
    "```\n",
    "DEBUG:composer.trainer.trainer:CUDA out of memory detected.\n",
    "Gradient Accumulation increased from 1 -> 2, and the batch\n",
    "will be retrained.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with different batch sizes and image sizes, and notice the trainer will never hit OutOfMemory errors, and you do not have to manually tweak the gradient accumulation to get the model to fit!\n",
    "\n",
    "For more details, see our [Auto Grad Accum](https://docs.mosaicml.com/en/latest/notes/auto_grad_accum.html) documentation."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
