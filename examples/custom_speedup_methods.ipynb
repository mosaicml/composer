{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ¤– Custom Speedup Methods\n",
    "\n",
    "This notebook is intended to show the process of implementing a custom method to work with the composer trainer. In order, it will cover:\n",
    "* Implementing a method using standard PyTorch\n",
    "* Brief overview of events and state in Composer\n",
    "* Modifying the implementation to work with Composer\n",
    "* Composing multiple methods together\n",
    "\n",
    "We should see that getting a method working with Composer is not very different from getting it working with vanilla PyTorch; we just need to wrap it with a few extra things so that Composer knows how to run it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Composer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, installation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install mosaicml matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a method with pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll go through the process of implementing a new method without using composer. First, some relevant imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, set up some training data. For simplicity, we will use CIFAR10 with minimal preprocessing. All we do is convert elements to tensors and normalize them using a randomly chosen mean and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = (0.507, 0.487, 0.441)\n",
    "std = (0.267, 0.256, 0.276)\n",
    "\n",
    "c10_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_dataset = datasets.CIFAR10('./data', train=True, download=True, transform=c10_transforms)\n",
    "test_dataset = datasets.CIFAR10('./data', train=False, download=True, transform=c10_transforms)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1024, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will define a model. For this, we will simply use Composer's ResNet56. One quirk to be aware of with this model is that the forward method takes in an `(X, y)` pair of inputs and targets, essentially what the dataloaders will spit out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from composer.models import composer_resnet_cifar\n",
    "\n",
    "model = composer_resnet_cifar(model_name='resnet_56', num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll define a function to test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model((data, target))\n",
    "            output = F.log_softmax(output, dim=1)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(test_loss, correct, len(test_loader.dataset),\n",
    "          100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will train it for a single epoch to check that things are working. We'll also test before and after training to check how accuracy changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "test(model, test_dataloader, device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "model.train()\n",
    "for batch_idx, (data, target) in enumerate(train_dataloader):\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    output = model((data, target))\n",
    "    output = F.log_softmax(output, dim=1)\n",
    "    loss = F.nll_loss(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "test(model, test_dataloader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like things are working! Time to implement our own modification to the training procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "For this tutorial, we'll look at how to implement one of the simpler speedup methods currently in our composer library: [ColOut](https://docs.mosaicml.com/en/stable/method_cards/col_out.html). This method works on image data by dropping random rows and columns from the training images. This reduces the size of the training images, which reduces the time per training iteration and hopefully does not alter the semantic content of the image too much. Additionally, dropping a small fraction of random rows and columns can also slightly distort objects and perhaps provide a data augmentation effect.\n",
    "\n",
    "To start our implementation, we'll write a function to drop random rows and columns from a batch of input images. We'll assume that these are torch tensors and operate on a batch, rather than individual images, for simplicity here.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_colout(X, p_row, p_col):\n",
    "    # Get the dimensions of the image\n",
    "    row_size = X.shape[2]\n",
    "    col_size = X.shape[3]\n",
    "\n",
    "    # Determine how many rows and columns to keep\n",
    "    kept_row_size = int((1 - p_row) * row_size)\n",
    "    kept_col_size = int((1 - p_col) * col_size)\n",
    "\n",
    "    # Randomly choose indices to keep. Must be sorted for slicing\n",
    "    kept_row_idx = sorted(torch.randperm(row_size)[:kept_row_size].numpy())\n",
    "    kept_col_idx = sorted(torch.randperm(col_size)[:kept_col_size].numpy())\n",
    "\n",
    "    # Keep only the selected row and columns\n",
    "    X_colout = X[:, :, kept_row_idx, :]\n",
    "    X_colout = X_colout[:, :, :, kept_col_idx]\n",
    "    return X_colout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is very simple, but as a check, we should visualize what this does to the data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = next(iter(train_dataloader))\n",
    "\n",
    "X_colout_1 = batch_colout(X, 0.1, 0.1)\n",
    "X_colout_2 = batch_colout(X, 0.2, 0.2)\n",
    "X_colout_3 = batch_colout(X, 0.3, 0.3)\n",
    "\n",
    "def unnormalize(X, mean, std):\n",
    "    X *= torch.tensor(std).view(1, 3, 1, 1)\n",
    "    X += torch.tensor(mean).view(1, 3, 1, 1)\n",
    "    X = X.permute(0,2,3,1)\n",
    "    return X\n",
    "\n",
    "X = unnormalize(X, mean, std)\n",
    "X_colout_1 = unnormalize(X_colout_1, mean, std)\n",
    "X_colout_2 = unnormalize(X_colout_2, mean, std)\n",
    "X_colout_3 = unnormalize(X_colout_3, mean, std)\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20,5))\n",
    "axes[0].imshow(X[0])\n",
    "axes[0].set_title(\"Unmodified\", fontsize=18)\n",
    "axes[1].imshow(X_colout_1[0])\n",
    "axes[1].set_title(\"p_row = 0.1, p_col = 0.1\", fontsize=18)\n",
    "axes[2].imshow(X_colout_2[0])\n",
    "axes[2].set_title(\"p_row = 0.2, p_col = 0.2\", fontsize=18)\n",
    "axes[3].imshow(X_colout_3[0])\n",
    "axes[3].set_title(\"p_row = 0.3, p_col = 0.3\", fontsize=18)\n",
    "\n",
    "for ax in axes:\n",
    "    ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like things are behaving as they should! Now let's insert it into our training loop. We'll also reinitialize the model here for a fair comparison with our earlier, single epoch run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = composer_resnet_cifar(model_name='resnet_56', num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we perform colout on the batch of data that the dataloader spits out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "p_row = 0.15\n",
    "p_col = 0.15\n",
    "model.to(device)\n",
    "test(model, test_dataloader, device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "model.train()\n",
    "for batch_idx, (data, target) in enumerate(train_dataloader):\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    ### Insert ColOut here ###\n",
    "    data = batch_colout(data, p_row, p_col)\n",
    "    ### ------------------ ###\n",
    "    optimizer.zero_grad()\n",
    "    output = model((data, target))\n",
    "    output = F.log_softmax(output, dim=1)\n",
    "    loss = F.nll_loss(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "test(model, test_dataloader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is pretty similar, and the wall clock time is definitely faster. Perhaps this method does provide a speedup in this case, but to do a proper evaluation we would want to look at the pareto curves, similar to the data in our [explorer](https://app.mosaicml.com/explorer/imagenet).\n",
    "\n",
    "This style of implementation is similar to the functional implementations in the composer library. Since ColOut is already implemented there, we could have simply done:\n",
    "\n",
    "```python\n",
    "import composer.functional as cf\n",
    "data = cf.colout_batch(data, p_row, p_col)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A natural question here is how do we combine ColOut with other methods? One way to do so would be to simply repeat the process we went through above for each method we want to try, inserting it into the training loop where appropriate. However, this can quickly become unwieldy and makes it difficult to run experiments using many different combinations of methods. This is the problem Composer aims to solve. \n",
    "\n",
    "In the following sections, we will modify our above implementation to work with Composer so that we can run many methods together. The modifications we need to make are fairly simple. In essence, we just need a way to tell Composer where in the training loop to insert our method and track the appropriate objects our method acts on. With Composer, we can insert our method into the training loop at what Composer calls an `event` and track what our method needs to modify in what Composer calls `state`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Events and State in Composer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop Composer uses provides multiple different locations where method code can be inserted and run. These are called events. Diagramatically, the training loop looks as follows:\n",
    "\n",
    "![Training Loop](https://storage.googleapis.com/docs.mosaicml.com/images/training_loop.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the top, we see the different steps of the Composer training loop. At the bottom, we see the many events that occur at different places within the training loop. For example, the event `EVENT.BEFORE_FORWARD` occurs just before the forward pass through the model, but after dataloading and preprocessing has taken place. These events are the points at which we can run the code for the method we want to implement.\n",
    "\n",
    "Most methods require making modifications to some object used in training, such as the model itself, the input/output data, training hyperparameters, etc. These quantities are tracked in Composer's `State` object, which can be found [here](https://github.com/mosaicml/composer/blob/dev/composer/core/state.py). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a method with Composer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to set up our method to work with Composer's trainer. To do this we will wrap the ColOut transformation we wrote up above in a class that inherits from Composer's base `Algorithm` class. Then we will need to implement two methods within that class: a `match` method that tells composer which `event` we want ColOut to run on and an `apply` method that tells Composer how to run ColOut. First, some relevant imports from Composer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from composer import Trainer\n",
    "from composer.core import Algorithm, Event"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before, we inserted ColOut into the training loop after getting a batch from the dataloader, but before the forward pass. As such, it makes sense for us to run ColOut on the event `EVENT.AFTER_DATALOADER`. The `match` method for this will simply check that the current event is this one and return `True` if it is, and `False` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match(self, event, state):\n",
    "    return event == Event.AFTER_DATALOADER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `apply` method is also simple in this case. It will just tell Composer how to run the function we already wrote and how to save the results in `state`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply(self, event, state, logger):\n",
    "    inputs, labels = state.batch\n",
    "    new_inputs = batch_colout(\n",
    "        inputs,\n",
    "        p_row=self.p_row,\n",
    "        p_col=self.p_col\n",
    "    )\n",
    "    state.batch = (new_inputs, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Packaging this together into an algorithm class gives our full composer ready ColOut implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColOut(Algorithm):\n",
    "    def __init__(self, p_row=0.15, p_col=0.15):\n",
    "        self.p_row = p_row\n",
    "        self.p_col = p_col\n",
    "\n",
    "    def match(self, event, state):\n",
    "        return event == Event.AFTER_DATALOADER\n",
    "\n",
    "    def apply(self, event, state, logger):\n",
    "        inputs, labels = state.batch\n",
    "        new_inputs = batch_colout(\n",
    "            inputs,\n",
    "            p_row=self.p_row,\n",
    "            p_col=self.p_col\n",
    "        )\n",
    "        state.batch = (new_inputs, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create a model, optimizer and test it the same way as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = composer_resnet_cifar(model_name=\"resnet_56\", num_classes=10)\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to initialize the trainer. We'll have to give it our `model` and the two dataloaders. We'll tell the trainer to run for one epoch by setting `max_duration='1ep'` and run on the gpu by setting `device='gpu'`. Since we're handling the testing ourselves in this example, we'll turn off Composer's validation by setting `validate_every_n_epochs=-1` and set the seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    eval_dataloader=test_dataloader,\n",
    "    optimizers=optimizer,\n",
    "    max_duration='1ep',\n",
    "    eval_interval=0,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test(model, test_dataloader, device)\n",
    "trainer.fit()\n",
    "test(model, test_dataloader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to add ColOut. We'll recreate the model and optimizer and also create an instance of the ColOut class we implemented above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = composer_resnet_cifar(model_name=\"resnet_56\", num_classes=10)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "colout_method = ColOut()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All that's left now is to pass the `colout_method` object to the Composer `Trainer`, which is otherwise initialized the same way as before. To do this, we just need to set `algorithms=[colout_method]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    eval_dataloader=test_dataloader,\n",
    "    optimizers=optimizer,\n",
    "    max_duration='1ep',\n",
    "    algorithms=[colout_method],\n",
    "    eval_interval=0,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test(model, test_dataloader, device)\n",
    "trainer.fit()\n",
    "test(model, test_dataloader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Composing multiple methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll compose our custom ColOut method with another method from the composer library, `BlurPool`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from composer.algorithms.blurpool import BlurPool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the blurpool object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blurpool = BlurPool(\n",
    "    replace_convs=True,\n",
    "    replace_maxpools=True,\n",
    "    blur_first=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And add it to the list of methods for the trainer to run. In general, we can pass in as many methods as we want, and composer should know how to run them all together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = composer_resnet_cifar(model_name=\"resnet_56\", num_classes=10)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    eval_dataloader=test_dataloader,\n",
    "    optimizers=optimizer,\n",
    "    max_duration='1ep',\n",
    "    algorithms=[colout_method, blurpool],\n",
    "    eval_interval=0,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test(model, test_dataloader, device)\n",
    "trainer.fit()\n",
    "test(model, test_dataloader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other methods as examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above implementation is one of the simplest cases. For more complex methods, the process is the same, but might require using different `events`, or modifying different things in `state`. Here are some interesting examples of other methods in the composer library:\n",
    "\n",
    "[BlurPool](https://docs.mosaicml.com/en/stable/method_cards/blurpool.html) swaps out some of the layers of the network.\n",
    "\n",
    "[LayerFreezing](https://docs.mosaicml.com/en/stable/method_cards/layer_freezing.html) changes which network parameters are trained at different epochs.\n",
    "\n",
    "[RandAugment](https://docs.mosaicml.com/en/stable/method_cards/randaugment.html) Adds an additional data augmentation.\n",
    "\n",
    "[SelectiveBackprop](https://docs.mosaicml.com/en/stable/method_cards/selective_backprop.html) Changes which samples are used to compute gradients.\n",
    "\n",
    "[SAM](https://docs.mosaicml.com/en/stable/method_cards/sam.html) Changes the optimizer used for training.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
