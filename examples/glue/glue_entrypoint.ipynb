{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üö™ GLUE Entrypoint\n",
    "\n",
    "This notebook is intended to demonstrate how to use the [GLUE (General Language Understanding Evaluation)](https://gluebenchmark.com/)  entrypoint for pretraining NLP models and finetuning them on the 8 GLUE tasks. This will cover:\n",
    "\n",
    "* The basics of the entrypoint and what it enables\n",
    "* How to construct your YAML for training\n",
    "* Executing an example fine-tuning job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's get started and configure our environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "This tutorial requires access to an AWS S3 Bucket. If AWS credentials are not already available in your environment, then you will need to obtain an `AWS_ACCESS_KEY_ID` and an `AWS_SECRET_ACCESS_KEY` that have permission to upload to and download from a S3 Bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If necessary, uncomment the following lines to set AWS credentials\n",
    "\n",
    "# %env AWS_ACCESS_KEY_ID=*****\n",
    "# %env AWS_SECRET_ACCESS_KEY=***\n",
    "# %env AWS_DEFAULT_REGION=us-west-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Composer\n",
    "\n",
    "First, install Composer if you haven't already:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install mosaicml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics of the Entrypoint\n",
    "\n",
    "This entrypoint allows you to specify if you want to pretrain a NLP model, finetune a model on the downstream tasks, or run the entire pipeline. If pretraining, the entrypoint will handle distributed training across all available GPUs. If fine-tuning, the entrypoint will fine-tune all given checkpoints on all 8 GLUE tasks in parallel using multiprocessing pools. This entrypoint is designed to make this process more efficient and remove the tediousness of individually spawning jobs and manually loading all the model checkpoints. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing your YAML for training\n",
    "\n",
    "An full out-of-the-box YAML example for this entrypoint can be found in `./glue_example.yaml.` If you're already familiar with YAMLs, you can skip to the next part! If not, we'll break down how this is structured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-training\n",
    "\n",
    "If you are only pre-training an NLP model from scratch, you only need to specify the `pretrain_hparams` section of the YAML. In this section, you will find your standard hyperparameters for pretraining a model -- the model configuration, dataset and dataloader specifications, batch size, etc. For the default configuration, we use identical parameters to `composer/yamls/models/bert-base.yaml` to pretrain a BERT model. See [TrainerHparams documentation](https://docs.mosaicml.com/en/stable/api_reference/composer.trainer.trainer_hparams.html?highlight=trainerhparams#composer.trainer.trainer_hparams.TrainerHparams) for more information about what is included in these parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    pretrain_hparams:\n",
    "      # Use a bert-base model, initialized from scratch\n",
    "      model:\n",
    "        bert:\n",
    "          use_pretrained: false\n",
    "          tokenizer_name: bert-base-uncased\n",
    "          pretrained_model_name: bert-base-uncased\n",
    "\n",
    "      # Train the model on the English C4 corpus\n",
    "      train_dataset:\n",
    "        streaming_c4:\n",
    "          remote: s3://allenai-c4/mds/1/\n",
    "          local: /tmp/mds-cache/mds-c4/\n",
    "          split: train\n",
    "          shuffle: true\n",
    "          tokenizer_name: bert-base-uncased\n",
    "          max_seq_len: 128\n",
    "          group_method: truncate\n",
    "          mlm: true\n",
    "          mlm_probability: 0.15\n",
    "\n",
    "      dataloader:\n",
    "        pin_memory: true\n",
    "        timeout: 0\n",
    "        prefetch_factor: 2\n",
    "        persistent_workers: true\n",
    "        num_workers: 8\n",
    "\n",
    "      # Periodically evaluate the LanguageCrossEntropy and Masked Accuracy\n",
    "      # on the validation split of the dataset.\n",
    "      evaluators:\n",
    "        evaluator:\n",
    "            label: bert_pre_training\n",
    "            eval_dataset:\n",
    "              streaming_c4:\n",
    "                remote: s3://allenai-c4/mds/1/\n",
    "                local: /tmp/mds-cache/mds-c4/\n",
    "                split: val\n",
    "                shuffle: false\n",
    "                tokenizer_name: bert-base-uncased\n",
    "                max_seq_len: 128\n",
    "                group_method: truncate\n",
    "                mlm: true\n",
    "                mlm_probability: 0.15\n",
    "            metric_names:\n",
    "              - LanguageCrossEntropy\n",
    "              - MaskedAccuracy\n",
    "\n",
    "      # Run evaluation after every 1000 training steps\n",
    "      eval_interval: 1000ba\n",
    "\n",
    "      # Use the decoupled AdamW optimizer with learning rate warmup\n",
    "      optimizers:\n",
    "        decoupled_adamw:\n",
    "          lr: 5.0e-4                     # Peak learning rate\n",
    "          betas:\n",
    "            - 0.9\n",
    "            - 0.98\n",
    "          eps: 1.0e-06\n",
    "          weight_decay: 1.0e-5           # Amount of weight decay regularization\n",
    "      schedulers:\n",
    "        linear_decay_with_warmup:\n",
    "          t_warmup: 0.06dur              # Point when peak learning rate is reached\n",
    "          alpha_f: 0.02\n",
    "\n",
    "      max_duration: 275184000sp          # Subsample the training data for 275M samples\n",
    "      train_batch_size: 4000             # Number of training examples to use per update\n",
    "      eval_batch_size: 2000\n",
    "\n",
    "      precision: amp                     # Use mixed-precision training\n",
    "      grad_clip_norm: -1.0               # Turn off gradient clipping\n",
    "      grad_accum: 'auto'                 # Use automatic gradient accumulation to avoid OOMs\n",
    "\n",
    "      save_folder: checkpoints           # The directory to save checkpoints to\n",
    "      save_interval: 3500ba              # Save checkpoints every 3500 batches\n",
    "      save_artifact_name: '{run_name}/checkpoints/ep{epoch}-ba{batch}-rank{rank}' \n",
    "      save_num_checkpoints_to_keep: 0\n",
    "      save_overwrite: True\n",
    "\n",
    "      loggers:\n",
    "        object_store:\n",
    "          object_store_hparams:         # The bucket to save checkpoints to\n",
    "            s3:\n",
    "              bucket: your-bucket-here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning\n",
    "\n",
    "If you are only fine-tuning checkpoints on the GLUE tasks, you are expected to specify the checkpoints to load from by specifying a `finetune_ckpts` list as so in the `finetune_hparams` section of your YAML. Upon runnning the entrypoint with this list, it will automatically pull all checkpoints and finetune on all of them. Note that if the `finetune_ckpts` list contains paths in object store, the entrypoint expects an `ObjectStoreLogger` and `load_object_store` instances, as well as their corresponding credentials to be specified, otherwise it will try to load from local disk. See our [checkpointing guide](https://docs.mosaicml.com/en/stable/trainer/checkpointing.html#resume-training) if you're not familiar with our checkpoint saving and loading schema. \n",
    "\n",
    "In all logging instances, such as Weights and Biases and in the results table outputted at the end of training, all the fine-tune runs will be grouped by pre-train checkpoint name for easier organization and run tracking.\n",
    "\n",
    "Below is an example `finetune_hparams` that loads checkpoints from an Amazon S3 bucket:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    finetune_hparams:\n",
    "      ...\n",
    "      finetune_ckpts:\n",
    "        - path/to/checkpoint1\n",
    "        - path/to/checkpoint2\n",
    "\n",
    "      # if paths are in ObjectStore, the below is expected to be defined\n",
    "      loggers:\n",
    "        object_store:\n",
    "          object_store_hparams: &bucket                 # Aliases the bucket information under bucket\n",
    "            s3:\n",
    "              bucket: your-bucket-here\n",
    "      load_object_store:\n",
    "        *bucket                                         # Refers to the previously defined bucket alias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "‚ùó Note: The load paths provided in `finetune_ckpts` have to be relative paths within an object store bucket/local directory as Composer does not currently allow checkpoints to be loaded via remote URIs.  Alternatively, you can provide a full https URL to a remote checkpoint as your full path, such as `https://storage.googleapis.com/path/to/checkpoint.pt`, and you will no longer need to define the `load_object_store` parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-training and fine-tuning\n",
    "\n",
    "To run the entire end-to-end pipeline, you are expected to provide the entrypoint with your pre-train configuration as explained above, as well as any overrides to apply to the fine-tuning jobs. In this case, the entrypoint is run in two distinct stages for distributed pre-training and multiprocessed fine-tuning; however, all information transferred between the stages is automatically handled by the entrypoint.  Checkpoints are automatically saved to your specified `save_folder` and loaded from wherever pre-training saved them, therefore the `finetune_ckpts` section of `finetune_hparams` is ignored if specified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùó Note: The entrypoint runs all 8 GLUE fine-tuning tasks on every saved pre-training checkpoint, so set your [save_interval](https://docs.mosaicml.com/en/stable/trainer/checkpointing.html?highlight=sve%20interval#save-interval) within your `pretrain_hparams` appropriately to avoid unnecessarily long evaluation times. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing your jobs\n",
    "\n",
    "Let's now put together all our knowledge about the entrypoint and launch a job that will fine-tune a BERT model on the 8 GLUE tasks! Because we are only fine-tuning with no special configurations, we only need to specify our bucket information and the `finetune_ckpts` to load from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = f\"\"\"\\\n",
    "finetune_hparams:\n",
    "  # defaults\n",
    "  loggers:\n",
    "    object_store:\n",
    "      object_store_hparams: &bucket\n",
    "        s3:\n",
    "          bucket: mosaicml-internal-checkpoints-bert\n",
    "  load_object_store: \n",
    "    *bucket\n",
    "  save_folder: checkpoints\n",
    "  finetune_ckpts:\n",
    "    - bert-baseline-tokenizer-2uoe/checkpoints/ep0-ba68796-rank0\n",
    "  \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now dump our constructed hparams to a YAML file to be loaded by the entrypoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import tempfile\n",
    "\n",
    "tmp_file = tempfile.NamedTemporaryFile()\n",
    "with open(tmp_file.name, 'w+') as f:\n",
    "    yaml.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's launch it! At the end of training, we will see a table containing the GLUE per-task, GLUE-Large, and GLUE-All scores!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python run_glue_trainer.py -f {tmp_file.name} --training_scheme finetune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùó Note: You must run this entrypoint via the **python** command, not the composer one!\n",
    "\n",
    "üí° Pro-tip: Try `python examples/glue/run_glue_trainer.py --help` to get more information about the entrypoint, and `python examples/glue/run_glue_trainer.py {pretrain_hparams, finetune_hparams} --help` to get a detailed breakdown of your hparams options!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "Try pre-training and fine-tuning your own models with this framework! Also, feel free to check out the rest of our [Composer docs](https://docs.mosaicml.com/) to try using our speedups in this entrypoint!\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
