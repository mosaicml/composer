{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "4a6d636e9c5640d496b4108d6487bac2",
    "deepnote_cell_height": 84,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "# This is a write-up for how to do distributed training using composer with \"submitit\" on the SLURM cluster\n",
    "# The idea is to put the üïπÔ∏è / üèí Distributed Training with Submitit tutorial as a subsection under üë®‚Äçüë©‚Äçüëß‚Äçüë¶ Distributed Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "c0d4b5793d0e4b2eb2f4f4fc33ea7d7c",
    "deepnote_cell_height": 82,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "# üë®‚Äçüë©‚Äçüëß‚Äçüë¶ Distributed Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "113c4d9346b941aabef329273fe27c98",
    "deepnote_cell_height": 235.171875,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "\n",
    "For additional configurations of our launcher script, run composer --help.\n",
    "\n",
    "```\n",
    "usage: composer [-h] [--version] [-n NPROC] [--stdout STDOUT]\n",
    "                [--stderr STDERR] [-v] [-m] [-c] [--world_size WORLD_SIZE]\n",
    "                [--base_rank BASE_RANK] [--node_rank NODE_RANK]\n",
    "                [--master_addr MASTER_ADDR] [--master_port MASTER_PORT]\n",
    "                training_script ...\n",
    "```\n",
    "\n",
    "It's also possible to initialize the environment variables without using the composer launcher. For more details, see üïπÔ∏è / üèí Distributed Training with Submitit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "fa01c6233eb141b18c7a7ffbcd127d14",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "# üïπÔ∏è Distributed Training with Submitit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "e90ac6d5536846758101a642b79585d5",
    "deepnote_cell_height": 174.390625,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Composer is compatible with [submitit](https://github.com/facebookincubator/submitit), a lightweight SLURM cluster job management package with Python API. To run distributed training on SLURM with submitit, the following environment variable need to be specified:\n",
    "\n",
    "```\n",
    "RANK, WORLD_SIZE, LOCAL_RANK, LOCAL_WORLD_SIZE, NODE_RANK, MASTER_ADDR, MASTER_PORT, PYTHONUNBUFFERED\n",
    "```\n",
    "\n",
    "In this tutorial, we walk through how to set up these environment variables without using the composer launcher. The example task are considering is standard supervised training of ResNet18 on ImageNet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0a9cca48c92f447d93c68b84eeac363a",
    "deepnote_cell_height": 70,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Prerequisite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "308c2201c44240d7b8a37e460f30fad9",
    "deepnote_cell_height": 52.390625,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "To start with, let's first install the composer library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "1a54499f0b9545b5a1501f5a6fb3a93a",
    "deepnote_cell_height": 61,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "%pip install mosaicml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "0a6d7c5f16014b68a1f803ba18ddbc48",
    "deepnote_cell_height": 61,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "import composer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "a2317dc5560d4c17ba6a292355002508",
    "deepnote_cell_height": 70,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "b645962d07e94a80acd7376be09996df",
    "deepnote_cell_height": 52.390625,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "We assume the standard pytorch dataloader for the composer training pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "84ea4256404349be8869c7d017bdb862",
    "deepnote_cell_height": 115,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "8652e444f7e244febd367a18ee7a14f7",
    "deepnote_cell_height": 97,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "def initialize_dataset():\n",
    "    cifar10_transform = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010]),]\n",
    "    )\n",
    "    train_data = torchvision.datasets.CIFAR10(\n",
    "        root=\".\", train=True, download=True, transform=cifar10_transform\n",
    "    )\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_data,\n",
    "        sampler=None,\n",
    "        batch_size=1024,\n",
    "        num_workers=10,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    return train_loader  # standard pytorch dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "e0c239bc455e4dbb8402c19bb9bfb906",
    "deepnote_cell_height": 70,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Prepare Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "fff3b797982b46a689843281f1477cc5",
    "deepnote_cell_height": 74.796875,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "To use the composer Trainer, our model must be wrapped in the ComposerModel class. Here is a quick demo for the model wrapper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "21348c218fe84050b4eeb5bf72c6c972",
    "deepnote_cell_height": 421,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from composer.models import ComposerModel\n",
    "\n",
    "class ResNet18(ComposerModel):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = torchvision.models.resnet18()\n",
    "\n",
    "    def forward(self, batch): # batch is the output of the dataloader\n",
    "        # specify how batches are passed through the model\n",
    "        inputs, _ = batch\n",
    "        return self.model(inputs)\n",
    "\n",
    "    def loss(self, outputs, batch):\n",
    "        # pass batches and `forward` outputs to the loss\n",
    "        _, targets = batch\n",
    "        return F.cross_entropy(outputs, targets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "c0133067f82841e5bc61d3c6f6fe4f9a",
    "deepnote_cell_height": 52.390625,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "For more details about model wrapping, see [üõª ComposerModel](https://docs.mosaicml.com/en/stable/composer_model.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "9f135416fba74637be4d214974d730b6",
    "deepnote_cell_height": 52.390625,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Next, we initialize a standard Adam optimizer from pytorch for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "5c343f91013f4c7dbfe52532fecb97e6",
    "deepnote_cell_height": 151,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def initialize_model_and_optimizer():\n",
    "    model = ResNet18()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "804622d72fd84bb6a430ad53c579b700",
    "deepnote_cell_height": 70,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Set up Environment Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "c2f089360bcc4032ae0533c3708a99a6",
    "deepnote_cell_height": 97.1875,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Before training, we need to set up all the necessary environment variables to correctly initialize the distributed training process group. The environment variables can be set using submitit built-in attributes, torch methods, and existing environment variables generated by the SLURM cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "94aa92db13bf40408f93adb1f7057846",
    "deepnote_cell_height": 97,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import submitit\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "e0f13f39a2754ddb84c949065f717878",
    "deepnote_cell_height": 655,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "def set_up_dist_env():\n",
    "    # 1. RANK\n",
    "    job_env = submitit.JobEnvironment()\n",
    "    global_rank = job_env.global_rank\n",
    "    \n",
    "    # 2. LOCAL_RANK\n",
    "    local_rank = job_env.local_rank\n",
    "    \n",
    "    # 3. LOCAL_WORLD_SIZE\n",
    "    ngpus_per_node = torch.cuda.device_count()\n",
    "    \n",
    "    # 4. WORLD_SIZE\n",
    "    world_size = int(os.getenv(\"SLURM_NNODES\")) * ngpus_per_node\n",
    "    \n",
    "    # 5. NODE_RANK\n",
    "    node_rank = int(os.getenv(\"SLURM_NODEID\"))\n",
    "    \n",
    "    # 6. MASTER_ADDR\n",
    "    cmd = \"scontrol show hostnames \" + os.getenv(\"SLURM_JOB_NODELIST\")\n",
    "    stdout = subprocess.check_output(cmd.split())\n",
    "    host_name = stdout.decode().splitlines()[0]\n",
    "    \n",
    "    # 7. MASTER_PORT\n",
    "    port = 54321\n",
    "    \n",
    "    # Set All the Necessary Environment Variables!\n",
    "    os.environ[\"RANK\"] = str(global_rank)\n",
    "    os.environ[\"LOCAL_RANK\"] = str(local_rank)\n",
    "    os.environ[\"LOCAL_WORLD_SIZE\"] = str(ngpus_per_node)\n",
    "    os.environ[\"WORLD_SIZE\"] = str(world_size)\n",
    "    os.environ[\"NODE_RANK\"] = str(node_rank)\n",
    "    os.environ[\"MASTER_ADDR\"] = host_name\n",
    "    os.environ[\"MASTER_PORT\"] = str(port)\n",
    "    os.environ[\"PYTHONUNBUFFERED\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0175defd38474f56b60106aa613bfe20",
    "deepnote_cell_height": 399.96875,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "The above setup is a bare minimum version of the [composer launcher](https://github.com/mosaicml/composer/blob/dev/composer/cli/launcher.py) for the distributed training pipeline to work. Here is a code snipet from the [composer launcher](https://github.com/mosaicml/composer/blob/dev/composer/cli/launcher.py) source code about how composer sets the enviroment variables via argument parser. The above function is doing the same thing but with the necessary variables set automatically. \n",
    "\n",
    "\n",
    "```python\n",
    "# composer/composer/cli/launcher.py\n",
    "\n",
    "with _patch_env(\n",
    "        RANK=str(global_rank),\n",
    "        WORLD_SIZE=str(world_size),\n",
    "        LOCAL_RANK=str(local_rank),\n",
    "        LOCAL_WORLD_SIZE=str(nproc),\n",
    "        NODE_RANK=str(node_rank),\n",
    "        MASTER_ADDR=master_addr,\n",
    "        MASTER_PORT=str(master_port),\n",
    "        PYTHONUNBUFFERED='1',\n",
    "):\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "3ddb24c42c3a42fe8377cdb2334bf8fc",
    "deepnote_cell_height": 70,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Submit Job to the Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "ec3fea1ee71145828bddb5aecf69f2db",
    "deepnote_cell_height": 111.1875,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Here comes the final step. Assume we have a multi-node setup, where we have two nodes and each node has four GPUs. The same `set_up_dist_env()` function should also work with a single node setup with multiple GPUs.\n",
    "\n",
    "\n",
    "Let's define the `submit_job()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "3f01decae4a84f4d9332c23a68c246a6",
    "deepnote_cell_height": 349,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "def submit_job():\n",
    "    slurm_ngpus = 4\n",
    "    slurm_nnodes = 2\n",
    "    slurm_timeout = 1024\n",
    "    workers = 10\n",
    "    \n",
    "    slurm_directory = \".\" # \"<Your Specified Directory>\"\n",
    "    executor = submitit.AutoExecutor(folder=slurm_directory)\n",
    "\n",
    "    executor.update_parameters(\n",
    "            mem_gb=128*slurm_ngpus,\n",
    "            gpus_per_node=slurm_ngpus,\n",
    "            tasks_per_node=slurm_ngpus,\n",
    "            cpus_per_task=workers,\n",
    "            nodes=slurm_nnodes,\n",
    "            timeout_min=slurm_timeout,\n",
    "            slurm_partition=\"gpu\",\n",
    "            # see submitit github repo for details\n",
    "    )\n",
    "\n",
    "    job = executor.submit(train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "56a8162da7b44113a5003db41346153f",
    "deepnote_cell_height": 70,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Putting Things Together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "8439f45c91a34f8dac6241891caba285",
    "deepnote_cell_height": 52.390625,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "we can now put everything into a python file for job submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_id": "97a7e830aad24abc8ef1333bcbc51951",
    "deepnote_cell_height": 907,
    "deepnote_cell_type": "code"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/home/ykuang/miniconda3/envs/mcmc2/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/mnt/home/ykuang/miniconda3/envs/mcmc2/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /mnt/home/ykuang/miniconda3/envs/mcmc2/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZNK3c1010TensorImpl36is_contiguous_nondefault_policy_implENS_12MemoryFormatE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import submitit\n",
    "import subprocess\n",
    "\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageNet\n",
    "\n",
    "import composer\n",
    "from composer.models import ComposerModel\n",
    "from composer import Trainer\n",
    "\n",
    "class ResNet18(ComposerModel):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = torchvision.models.resnet18()\n",
    "\n",
    "    def forward(self, batch): # batch is the output of the dataloader\n",
    "        # specify how batches are passed through the model\n",
    "        inputs, _ = batch\n",
    "        return self.model(inputs)\n",
    "\n",
    "    def loss(self, outputs, batch):\n",
    "        # pass batches and `forward` outputs to the loss\n",
    "        _, targets = batch\n",
    "        return F.cross_entropy(outputs, targets)\n",
    "\n",
    "def initialize_model_and_optimizer():\n",
    "    model = ResNet18()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "    return model, optimizer\n",
    "\n",
    "def initialize_dataset():\n",
    "    cifar10_transform = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010]),]\n",
    "    )\n",
    "    train_data = torchvision.datasets.CIFAR10(\n",
    "        root=\".\", train=True, download=True, transform=cifar10_transform\n",
    "    )\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_data,\n",
    "        sampler=None,\n",
    "        batch_size=1024,\n",
    "        num_workers=10,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    return train_loader  # standard pytorch dataloader\n",
    "\n",
    "def set_up_dist_env():\n",
    "    # 1. RANK\n",
    "    job_env = submitit.JobEnvironment()\n",
    "    global_rank = job_env.global_rank\n",
    "    \n",
    "    # 2. LOCAL_RANK\n",
    "    local_rank = job_env.local_rank\n",
    "    \n",
    "    # 3. LOCAL_WORLD_SIZE\n",
    "    ngpus_per_node = torch.cuda.device_count()\n",
    "    \n",
    "    # 4. WORLD_SIZE\n",
    "    world_size = int(os.getenv(\"SLURM_NNODES\")) * ngpus_per_node\n",
    "    \n",
    "    # 5. NODE_RANK\n",
    "    node_rank = int(os.getenv(\"SLURM_NODEID\"))\n",
    "    \n",
    "    # 6. MASTER_ADDR\n",
    "    cmd = \"scontrol show hostnames \" + os.getenv(\"SLURM_JOB_NODELIST\")\n",
    "    stdout = subprocess.check_output(cmd.split())\n",
    "    host_name = stdout.decode().splitlines()[0]\n",
    "    \n",
    "    # 7. MASTER_PORT\n",
    "    port = 54321\n",
    "    \n",
    "    # Set All the Necessary Environment Variables!\n",
    "    os.environ[\"RANK\"] = str(global_rank)\n",
    "    os.environ[\"LOCAL_RANK\"] = str(local_rank)\n",
    "    os.environ[\"LOCAL_WORLD_SIZE\"] = str(ngpus_per_node)\n",
    "    os.environ[\"WORLD_SIZE\"] = str(world_size)\n",
    "    os.environ[\"NODE_RANK\"] = str(node_rank)\n",
    "    os.environ[\"MASTER_ADDR\"] = host_name\n",
    "    os.environ[\"MASTER_PORT\"] = str(port)\n",
    "    os.environ[\"PYTHONUNBUFFERED\"] = \"1\"\n",
    "\n",
    "def train():\n",
    "    set_up_dist_env()\n",
    "    train_dataloader = initialize_dataset()\n",
    "    model, optimizer = initialize_model_and_optimizer()\n",
    "    \n",
    "    print(\"Trainer\")\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        optimizers=optimizer,\n",
    "        train_dataloader=train_dataloader,\n",
    "        max_duration='10ep',\n",
    "        device=\"gpu\",\n",
    "    )\n",
    "    print(\"trainer.fit\")\n",
    "    trainer.fit()\n",
    "\n",
    "def submit_job():\n",
    "    slurm_ngpus = 4\n",
    "    slurm_nnodes = 2\n",
    "    slurm_timeout = 1024\n",
    "    workers = 10\n",
    "    \n",
    "    slurm_directory = \".\" # \"<Your Specified Directory>\"\n",
    "    executor = submitit.AutoExecutor(folder=slurm_directory)\n",
    "\n",
    "    executor.update_parameters(\n",
    "            mem_gb=128*slurm_ngpus,\n",
    "            gpus_per_node=slurm_ngpus,\n",
    "            tasks_per_node=slurm_ngpus,\n",
    "            cpus_per_task=workers,\n",
    "            nodes=slurm_nnodes,\n",
    "            timeout_min=slurm_timeout,\n",
    "            slurm_partition=\"gpu\",\n",
    "            # see submitit github repo for details\n",
    "    )\n",
    "\n",
    "    job = executor.submit(train)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    submit_job()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "1f0fb2c92b6e4887a718529479622623",
    "deepnote_cell_height": 52.390625,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Run the above python script in your command shell and you're all set! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=4af6be4e-8c34-4042-9ba3-64b51038fd7e' target=\"_blank\">\n",
    "<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n",
    "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
   ]
  }
 ],
 "metadata": {
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "851418bcf18f4d14b054f1beb28ec800",
  "kernelspec": {
   "display_name": "kernel_ssl",
   "language": "python",
   "name": "kernel_ssl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
