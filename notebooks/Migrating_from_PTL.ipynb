{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Migrating from PyTorch Lightning\n",
    "\n",
    "[PyTorch Lightning](https://www.pytorchlightning.ai/) is a popular and very well designed framework for training deep learning models. If you are interested in trying our efficient algorithms and using the Composer trainer, the below is a quick guide on how to adapt your models. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are running in Colab, or haven't installed composer yet, do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mosaicml \n",
    "!pip install pytorch-lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll go through the process of migrating the Resnet18 on CIFAR10 model from PTL to composer. We will be following the PTL example [here](https://pytorch-lightning.readthedocs.io/en/stable/notebooks/lightning_examples/cifar10-baseline.html). \n",
    "\n",
    "First, some relevant imports, as well as creating the model as in the PTL tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pytorch_lightning import LightningModule\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "def create_model():\n",
    "    model = torchvision.models.resnet18(pretrained=False, num_classes=10)\n",
    "    model.conv1 = nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "    model.maxpool = nn.Identity()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is standard, we setup the training data for CIFAR10 using `torchvision` datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "transform = torchvision.transforms.Compose(\n",
    "    [torchvision.transforms.ToTensor(),\n",
    "     torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(trainset, batch_size=256, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(testset, batch_size=256, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing PTL Lightning Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the PTL tutorial, we use the `LitResnet` model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitResnet(LightningModule):\n",
    "    \n",
    "    def __init__(self, lr=0.05):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "        self.model = create_model()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        return F.log_softmax(out, dim=1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def evaluate(self, batch, stage=None):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = accuracy(preds, y)\n",
    "\n",
    "        if stage:\n",
    "            self.log(f\"{stage}_loss\", loss, prog_bar=True)\n",
    "            self.log(f\"{stage}_acc\", acc, prog_bar=True)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self.evaluate(batch, \"val\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        self.evaluate(batch, \"test\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.SGD(\n",
    "            self.parameters(),\n",
    "            lr=self.hparams.lr,\n",
    "            momentum=0.9,\n",
    "            weight_decay=5e-4,\n",
    "        )\n",
    "        steps_per_epoch = 45000 // 256\n",
    "        scheduler_dict = {\n",
    "            \"scheduler\": OneCycleLR(\n",
    "                optimizer,\n",
    "                0.1,\n",
    "                epochs=30,\n",
    "                steps_per_epoch=steps_per_epoch,\n",
    "            ),\n",
    "            \"interval\": \"step\",\n",
    "        }\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler_dict}\n",
    "\n",
    "PTLModel = LitResnet(lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lightning module to Composer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that up to here, we have only used pytorch lightning code. Here we will transfer the PTL module to be compatible with composer. There are a few major differences:\n",
    "* The `training_step` is broke into two parts, the `forward` and the `loss` methods. This is needed since our algorithms sometimes (such as label smoothing or selective backprop) needs to intercept and modify the loss. \n",
    "* Optimizers and schedulers are passed directly to the Trainer during initialization.\n",
    "* Our `forward` step accepts as input the entire batch, and has to take care of unpacking the batch. \n",
    "\n",
    "For more information about the `ComposerModel` format, see our [guide](https://docs.mosaicml.com/en/stable/composer_model.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.classification.accuracy import Accuracy\n",
    "from composer import ComposerModel \n",
    "\n",
    "class MosaicResnet(ComposerModel):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.model = create_model()\n",
    "        self.acc = Accuracy()\n",
    "        super().__init__()\n",
    "\n",
    "    def loss(self, outputs, batch, *args, **kwargs):\n",
    "        \"\"\"Accepts the outputs from forward() and the batch\"\"\"\n",
    "        _, y = batch  # unpack the labels\n",
    "        return F.nll_loss(outputs, y)\n",
    "\n",
    "    def metrics(self, train):\n",
    "        return self.acc\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, _ = batch\n",
    "        return F.log_softmax(self.model(x), dim=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instantiate the Mosaic trainer similarly by specifying the model, dataloaders, optimizers, max_duration (epochs). For more details on the trainer arguments, see [Using the Trainer](https://docs.mosaicml.com/en/stable/trainer/using_the_trainer.html) guide.\n",
    "\n",
    "Now, you are ready to insert your algorithms! As an example here, we add a few common algorithms -- [Label Smoothing](https://docs.mosaicml.com/en/latest/method_cards/label_smoothing.html) and [BlurPool](https://docs.mosaicml.com/en/latest/method_cards/blurpool.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from composer.algorithms import LabelSmoothing, BlurPool\n",
    "from composer import Trainer\n",
    "\n",
    "trainer = Trainer(model=MosaicResnet(),  \n",
    "                  algorithms=[LabelSmoothing(alpha=0.1), BlurPool()],\n",
    "                  train_dataloader=train_dataloader,\n",
    "                  eval_dataloader=test_dataloader,\n",
    "                  optimizers=PTLmodel.configure_optimizers()[\"optimizer\"],\n",
    "                  schedulers=PTLmodel.configure_optimizers()[\"lr_scheduler\"][\"scheduler\"],\n",
    "                  step_schedulers_every_batch=True,  # interval should be step                  \n",
    "                  max_duration='1ep',\n",
    ")\n",
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Migrating from PTL.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
