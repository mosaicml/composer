{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Migrating from PTL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is intended to demonstrate migrating a model from PTL to composer. We \n"
      ],
      "metadata": {
        "id": "3a_vi5Z0jUCG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install composer"
      ],
      "metadata": {
        "id": "zuzRxREkfXxI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, installation"
      ],
      "metadata": {
        "id": "8yMyxYl0aG6q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing from a branch until main is updated\n",
        "!pip install git+https://github.com/mosaicml/composer.git@dev"
      ],
      "metadata": {
        "id": "UXT_drm1jKCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting started\n",
        "\n"
      ],
      "metadata": {
        "id": "B40gf4aYi92X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we'll go through the process of migrating the Resnet18 on CIFAR10 model from PTL to composer.\n",
        "\n",
        "We will be following the PTL example here https://pytorch-lightning.readthedocs.io/en/stable/notebooks/lightning_examples/cifar10-baseline.html.\n",
        "\n",
        " First, some relevant imports, as well as creating the model as in the PTL tutorial."
      ],
      "metadata": {
        "id": "gKpOBo3IPL38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from pytorch_lightning import LightningModule\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "\n",
        "from composer.models.base import ComposerModel\n",
        "from composer import Trainer\n",
        "\n",
        "def create_model():\n",
        "    model = torchvision.models.resnet18(pretrained=False, num_classes=10)\n",
        "    model.conv1 = nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
        "    model.maxpool = nn.Identity()\n",
        "    return model.cuda()"
      ],
      "metadata": {
        "id": "8rmUEyv6PV0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training data"
      ],
      "metadata": {
        "id": "wcuTiEvx_Mzk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, set up some training data. For demo simplicity, we will use CIFAR10 with minimal preprocessing rather than the PTL Datamodule as that needs extra dataset_train params."
      ],
      "metadata": {
        "id": "KPro2jbxt-sP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "transform = torchvision.transforms.Compose\n",
        "    [torchvision.transforms.ToTensor(),\n",
        "     torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='/localdisk/CIFAR10', train=True, download=True, transform=transform)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='/localdisk/CIFAR10', train=False, download=True, transform=transform)\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(trainset, batch_size=256, shuffle=True,\n",
        "                                               num_workers=2)  #cifar10_dm.train_dataloader()\n",
        "test_dataloader = torch.utils.data.DataLoader(testset, batch_size=256, shuffle=False, num_workers=2)\n"
      ],
      "metadata": {
        "id": "BCmsq5OevK9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "# Importing PTL Lightning Module"
      ],
      "metadata": {
        "id": "sDfVLcPj_Wxj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following the PTL tutorial, we use the LitResnet model."
      ],
      "metadata": {
        "id": "K8j1JKaS_q70"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LitResnet(LightningModule):\n",
        "    \n",
        "    def __init__(self, lr=0.05):\n",
        "        super().__init__()\n",
        "\n",
        "        self.save_hyperparameters()\n",
        "        self.model = create_model()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.model(x)\n",
        "        return F.log_softmax(out, dim=1)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = F.nll_loss(logits, y)\n",
        "        self.log(\"train_loss\", loss)\n",
        "        return loss\n",
        "\n",
        "    def evaluate(self, batch, stage=None):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = F.nll_loss(logits, y)\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        acc = accuracy(preds, y)\n",
        "\n",
        "        if stage:\n",
        "            self.log(f\"{stage}_loss\", loss, prog_bar=True)\n",
        "            self.log(f\"{stage}_acc\", acc, prog_bar=True)\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        self.evaluate(batch, \"val\")\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        self.evaluate(batch, \"test\")\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.SGD(\n",
        "            self.parameters(),\n",
        "            lr=self.hparams.lr,\n",
        "            momentum=0.9,\n",
        "            weight_decay=5e-4,\n",
        "        )\n",
        "        steps_per_epoch = 45000 // 256\n",
        "        scheduler_dict = {\n",
        "            \"scheduler\": OneCycleLR(\n",
        "                optimizer,\n",
        "                0.1,\n",
        "                epochs=30,\n",
        "                steps_per_epoch=steps_per_epoch,\n",
        "            ),\n",
        "            \"interval\": \"step\",\n",
        "        }\n",
        "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler_dict}"
      ],
      "metadata": {
        "id": "gbBGthLV_gEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lightning module to Composer"
      ],
      "metadata": {
        "id": "ULjnQklcqUdJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that up to here, we have only used PTL code. Here we will transfer the PTL module to composer. The ComposerModel needs 5 functions: __init__, loss, metrics, forward and validate. Everything else is under the hood. First we instatiate it:"
      ],
      "metadata": {
        "id": "3kkaIEU5q1wm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PTLmodel = LitResnet(lr=0.05)\n",
        "\n",
        "## Now we create the MosaicModel:\n",
        "\n",
        "class Mosaicmodel(ComposerModel):\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "    def loss(self, outputs, batch, *args, **kwargs):  # -> Tensors:\n",
        "        ## loss from PTL.training_step()\n",
        "        x, y = batch\n",
        "        return F.nll_loss(outputs, y)\n",
        "\n",
        "    def metrics(self, train):\n",
        "        ## acc from PTL.evaluate()\n",
        "        from torchmetrics.classification.accuracy import Accuracy\n",
        "        return Accuracy()\n",
        "\n",
        "    def forward(self, batch):\n",
        "        ## from PTL.forward()\n",
        "        x, _ = batch\n",
        "        return PTLmodel.forward(x)\n",
        "\n",
        "    def validate(self, batch):\n",
        "        ## from PTL.evaluate()\n",
        "        return PTLmodel.evaluate(batch, \"val\")"
      ],
      "metadata": {
        "id": "6XzOvcZkALQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "# And finally, training!"
      ],
      "metadata": {
        "id": "KqvN09r5AWyi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We instantiate the Mosaic trainer similarly by specifying the model, dataloaders, optimizers, max_duration (epochs). And then we fit, voila!"
      ],
      "metadata": {
        "id": "llKsUd7QAbQ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## COMPOSER TRAINER\n",
        "trainer = Trainer(model=Mosaicmodel().cuda(),\n",
        "                  train_dataloader=train_dataloader,\n",
        "                  eval_dataloader=test_dataloader,\n",
        "                  optimizers=PTLmodel.configure_optimizers()[\"optimizer\"],\n",
        "                  max_duration='1ep',\n",
        "                  device='gpu',\n",
        "                  validate_every_n_epochs=-1,\n",
        "                  seed=42)\n",
        "\n",
        "trainer.fit()"
      ],
      "metadata": {
        "id": "QcJGq1_GAsEY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}