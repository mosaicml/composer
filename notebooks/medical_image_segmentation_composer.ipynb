{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EvDLkyXIKcg8"
   },
   "source": [
    "In this notebook you will use composer and pytorch to segment pneumothorax (air around or outside of the lungs) from chest radiographic images. This dataset was originally released for a [kaggle competition](https://www.kaggle.com/c/siim-acr-pneumothorax-segmentation/overview) by the [Society for Informatics in Medicine](https://siim.org/) (SIIM).\n",
    "\n",
    "Disclaimer: This example represents a minimal working baseline. In order to get competitive results this notebook must run for a long time.\n",
    "\n",
    "\n",
    "We will cover:\n",
    "- installing relevant packages\n",
    "- downloading the SIIM dataset from kaggle\n",
    "- cleaning and resampling the dataset\n",
    "- splitting data for validation\n",
    "- visualizing model inputs\n",
    "- training a baseline model with composer\n",
    "- using composer methods\n",
    "- next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5KZuhZYKPG6o"
   },
   "source": [
    "# Environment and Data setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ds0u02aotM5V"
   },
   "source": [
    "## Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "If7O5P_T-eB2"
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/mosaicml/composer.git@dev --quiet # composer\n",
    "!pip install --upgrade --force-reinstall --no-deps kaggle --quiet # kaggle cli for accessing kaggle data\n",
    "!pip install pydicom --quiet # for reading dicom format medical images \n",
    "!pip install git+https://github.com/qubvel/segmentation_models.pytorch --quiet # easy segmentation models\n",
    "!pip install opencv-python-headless==4.1.2.30 --quiet #https://stackoverflow.com/questions/70537488/cannot-import-name-registermattype-from-cv2-cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vOXU0rGPNHt7"
   },
   "source": [
    "## Kaggle Authentication\n",
    "To access the data you need a Kaggle Account\n",
    "- accept competition terms https://www.kaggle.com/c/siim-acr-pneumothorax-segmentation/data\n",
    "- download kaggle.json from https://www.kaggle.com/yourusername/account\n",
    "- upload `kaggle.json` file using the following code cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uFTnRD52ZdKd"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CyruzpErrJjY"
   },
   "outputs": [],
   "source": [
    "! mkdir ~/.kaggle\n",
    "! mv kaggle.json ~/.kaggle/\n",
    "! chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kGdFmTaYzZHm"
   },
   "source": [
    "## Download and unzip the data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MXn9uUxxW0Wn"
   },
   "outputs": [],
   "source": [
    "!kaggle datasets download -d seesee/siim-train-test\n",
    "!unzip -q siim-train-test.zip -d .\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D2xeZNrxdqlh"
   },
   "source": [
    "## Flatten Image Directories\n",
    "The original dataset is oddly nested, we flatten it out so the images are easier to access in our pytorch dataset.\n",
    "\n",
    "`/siim/dicom-images-train/id/id/id.dcm` to `/siim/dicom-images-train/id.dcm`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "3ff5c34ab24b4629938408c7e83ca184",
      "62afe8a7c8024e3c90b1cb65dbe05605",
      "ce691b5958ca43beb35c7312125a9df7",
      "09ec07a854fc4b699da6a2533e2e105c",
      "c4e7c50f4724476d84c83fb71b520dc8",
      "cac458165c834216a454a3e3e709e1d0",
      "7c8c64e66821442a8ca73dd47ea45235",
      "69752d98aaea446d9636c53724b2d7e6",
      "0251f4a4e0f74fd788504e1360cb2dde",
      "8f8443cdd1834bf4a1e0ed890c7af6e6",
      "3bea6037e76146fdb31c158593089602"
     ]
    },
    "id": "L8tQI_fidpKQ",
    "outputId": "ec485a8c-fa3e-47f3-df4b-2419db9e9d94"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "train_images = list(Path('siim/dicom-images-train').glob('*/*/*.dcm'))\n",
    "for image in tqdm(train_images):\n",
    "    image.replace(f'siim/dicom-images-train/{image.parts[-1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IeK9GIRXPf3w"
   },
   "source": [
    "# Project setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kkuacHKb5h6w"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bYxam-Tuhoij"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from ipywidgets import interact, fixed, IntSlider\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "\n",
    "from pydicom.filereader import dcmread\n",
    "# model\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "# data\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.utils import draw_segmentation_masks, make_grid\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "# transforms\n",
    "from albumentations import ShiftScaleRotate, Resize, Compose\n",
    "\n",
    "from torchmetrics import Metric\n",
    "from torchmetrics.collections import MetricCollection\n",
    "\n",
    "\n",
    "from composer import Trainer\n",
    "from composer.optim import DecoupledAdamW\n",
    "from composer.optim.scheduler import WarmUpLR, CosineAnnealingLR\n",
    "from composer.models import ComposerModel\n",
    "from composer.models.loss import Dice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hShNYFabiNYn"
   },
   "source": [
    "## Utils\n",
    "Here we define some utility functions to help with logging, decoding/encoding targets and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SaME5wbvdrvp"
   },
   "outputs": [],
   "source": [
    "class LossMetric(Metric):\n",
    "    \"\"\"Turns any torch.nn Loss Module into distributed torchmetrics Metric.\"\"\"\n",
    "\n",
    "    def __init__(self, loss, dist_sync_on_step=False):\n",
    "        super().__init__(dist_sync_on_step=dist_sync_on_step)\n",
    "        self.loss = loss\n",
    "        self.add_state(\"sum_loss\", default=torch.tensor(0.), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"total_batches\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
    "\n",
    "    def update(self, preds, target):\n",
    "        \"\"\"Update the state with new predictions and targets.\n",
    "        \"\"\"\n",
    "        # Loss calculated over samples/batch, accumulate loss over all batches\n",
    "        self.sum_loss += self.loss(preds, target)\n",
    "        self.total_batches += 1\n",
    "\n",
    "    def compute(self):\n",
    "        \"\"\"Aggregate state over all processes and compute the metric.\n",
    "        \"\"\"\n",
    "        # Return average loss over entire validation dataset\n",
    "        return self.sum_loss / self.total_batches\n",
    "\n",
    "def rle2mask(rle, height=1024, width=1024, fill_value=1):\n",
    "    mask = np.zeros((height, width), np.float32)\n",
    "    mask = mask.reshape(-1)\n",
    "    rle = np.array([int(s) for s in rle.strip().split(' ')])\n",
    "    rle = rle.reshape(-1, 2)\n",
    "    start = 0\n",
    "    for index, length in rle:\n",
    "        start = start+index\n",
    "        end = start+length\n",
    "        mask[start: end] = fill_value\n",
    "        start = end\n",
    "    mask = mask.reshape(width, height).T\n",
    "    return mask\n",
    "\n",
    "def mask2rle(mask):\n",
    "    mask = mask.T.flatten()\n",
    "    start = np.where(mask[1:] > mask[:-1])[0]+1\n",
    "    end = np.where(mask[:-1] > mask[1:])[0]+1\n",
    "    length = end-start\n",
    "    rle = []\n",
    "    for i in range(len(length)):\n",
    "        if i == 0:\n",
    "            rle.extend([start[0], length[0]])\n",
    "        else:\n",
    "            rle.extend([start[i]-end[i-1], length[i]])\n",
    "    rle = ' '.join([str(r) for r in rle])\n",
    "    return rle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jR1fnDk-Vr7Z"
   },
   "source": [
    "# Preprocessing and Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xRry88pvi-ed"
   },
   "source": [
    "## SIIM Dataset\n",
    "\n",
    "The SIIM dataset consists of:\n",
    "- `dicom-images-train` - 12954 labeled images in [DICOM](https://pydicom.github.io/pydicom/stable/auto_examples/input_output/plot_read_dicom.) format.\n",
    "- `dicom-images-test`3205 unlabeled DICOM images for testing\n",
    "\n",
    "- `train-rle.csv` comes with a label file `train-rle.csv` mapping `ImageId` to `EncodedPixels`.\n",
    "\n",
    "    - `ImageId`s map to image paths for [DICOM](https://pydicom.github.io/pydicom/stable/auto_examples/input_output/plot_read_dicom.html#sphx-glr-auto-examples-input-output-plot-read-dicom-py) format images. \n",
    "\n",
    "  - `EncodedPixels` are [run length encoded](https://github.com/cocodataset/cocoapi/blob/master/PythonAPI/pycocotools/mask.py) segmentation masks representing areas where pneumothorax has labeled by an expert. A label of `\"-1\"` indicates the image was examined and no pneumothorax was found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gtDpnR1YRUlE",
    "outputId": "f206d43e-0032-40a6-f8f9-ba35819f645c"
   },
   "outputs": [],
   "source": [
    "!ls siim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZONFLJqzirHP",
    "outputId": "12a80287-2a36-4a00-b7fc-8900bd03f5a2"
   },
   "outputs": [],
   "source": [
    "labels_df = pd.read_csv('siim/train-rle.csv')\n",
    "labels_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0RDdNqgmV7Kj"
   },
   "source": [
    "## Clean Data\n",
    "Of the ~13,000 only 3600 have masks. We will throw out some of the negative samples to better balance our dataset and speed up training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "31EJ5NqlM6EK",
    "outputId": "812d31aa-6566-4913-9783-2a968db34dc3"
   },
   "outputs": [],
   "source": [
    "labels_df[labels_df[\" EncodedPixels\"] != \"-1\"].shape, labels_df[labels_df[\" EncodedPixels\"] == \"-1\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uJW2xrBlMXA1"
   },
   "outputs": [],
   "source": [
    "def balance_labels(labels_df, extra_samples_without_mask=1500, random_state=1337):\n",
    "  \"\"\"\n",
    "  Drop duplicates and mark samples with masks.\n",
    "  Sample 3576+extra_samples_without_mask unmasked samples to balance dataset.\n",
    "  \"\"\"\n",
    "    df = labels_df.drop_duplicates('ImageId')\n",
    "    df_with_mask = df[df[\" EncodedPixels\"] != \"-1\"].copy(deep=True)\n",
    "    df_with_mask['has_mask'] = 1\n",
    "    df_without_mask = df[df[\" EncodedPixels\"] == \"-1\"].copy(deep=True)\n",
    "    df_without_mask['has_mask'] = 0\n",
    "    df_without_mask_sampled = df_without_mask.sample(len(df_with_mask)+extra_samples_without_mask, random_state=random_state)\n",
    "    df = pd.concat([df_with_mask, df_without_mask_sampled])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T8fSWp9DWwB5",
    "outputId": "a63d5303-7c17-4743-efcf-55d631210599"
   },
   "outputs": [],
   "source": [
    "df = balance_labels(labels_df)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adO4mO6ePERE"
   },
   "source": [
    "## Create Cross Validation Splits\n",
    "Once cleaned at balanced, we're left with only 6838 images. Which will leave us with rather small training and validation sets once we split the data. To mitigate the chances of us validating on a poorly sampled (not representative of our unlabeled test data) validation set this, we use [StratifiedKFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html) to create 5 different 80%-20%, `train` `eval` splits. \n",
    "\n",
    "Disclaimer:\n",
    "\n",
    "For datasets of this size, it's good practice to train and evaluate on each split, but due to runtime constraints in this notebook we will only train on the first split which contains 5470 training and 1368 eval samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dbdC7mTyPCov"
   },
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=1337)\n",
    "train_idx, eval_idx = list(kfold.split(df[\"ImageId\"], df[\"has_mask\"]))[0]\n",
    "train_df, eval_df = df.iloc[train_idx], df.iloc[eval_idx]\n",
    "train_df.shape, eval_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MnvlEEkLmc6S"
   },
   "source": [
    "# Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f7QCmuagguc8"
   },
   "source": [
    "## Pytorch Dataset\n",
    "`SIIMDataset` is a standard pytorch dataset that reads images and decodes labels from the siim label csv. DICOM images are loaded as grayscale numpy arrays, converted to rgb, and scaled. Labels are converted from rle strings to binary segmentation masks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GlMl7Jema4Rz"
   },
   "outputs": [],
   "source": [
    "class SIIMDataset(Dataset):\n",
    "    def __init__(self, labels_df,\n",
    "                 transforms=None, image_dir=Path('siim/dicom-images-train')):\n",
    "        self.labels_df = labels_df\n",
    "        self.image_dir = image_dir\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.labels_df.iloc[idx]\n",
    "        image_id = row.ImageId\n",
    "        image_path = self.image_dir / f'{image_id}.dcm'\n",
    "        image = dcmread(image_path).pixel_array # load dicom image\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB) # convert rgb so we can keep imagenet first layer weights\n",
    "        image = (image / 255.).astype('float32') # scale (0.- 1.)\n",
    "\n",
    "        rle = row[' EncodedPixels']\n",
    "        if rle != '-1':\n",
    "            mask = rle2mask(rle, 1024, 1024).astype('float32')\n",
    "        else:\n",
    "            mask = np.zeros([1024, 1024]).astype('float32')\n",
    "\n",
    "        if self.transforms:\n",
    "            augmented = self.transforms(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "        return torch.from_numpy(image).permute(2, 0, 1), torch.from_numpy(mask).unsqueeze(0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XFXj5bcbFagV"
   },
   "source": [
    "## Transforms\n",
    "We use the [albumentations](https://albumentations.ai/docs/getting_started/mask_augmentation/) library to Resize, and randomly scale/rotate our training images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cQUImSwtYzEb"
   },
   "outputs": [],
   "source": [
    "image_size = 512\n",
    "\n",
    "train_transforms = Compose([Resize(image_size, image_size),\n",
    "                            ShiftScaleRotate(shift_limit=0,\n",
    "                                             scale_limit=0.1,\n",
    "                                             rotate_limit=10, # rotate\n",
    "                                             p=0.5,\n",
    "                                             border_mode=cv2.BORDER_CONSTANT)])\n",
    "\n",
    "eval_transforms = Compose([Resize(image_size, image_size)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ODau434Olm2t"
   },
   "source": [
    "## Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z8t2ELFdV-Wj"
   },
   "outputs": [],
   "source": [
    "\n",
    "train_batch_size = 32\n",
    "val_batch_size = 32\n",
    "\n",
    "train_dataloader = DataLoader(SIIMDataset(train_df, transforms=train_transforms),\n",
    "                              batch_size=train_batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "eval_dataloader = DataLoader(SIIMDataset(eval_df, transforms=eval_transforms),\n",
    "                             batch_size=val_batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qakt_KKQG9AS"
   },
   "source": [
    "## Visualize batch\n",
    "Areas of pneumothorax as highlighted in red, drag the slider to iterate through batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 899,
     "referenced_widgets": [
      "a1b55972432441d8870363e953ffd7e3",
      "3a95da793b424447a27ac5269e6d06c7",
      "37c5e831be654cb8a50d704405fd86d3",
      "89c0a9217cd7467bacdef3534acfbfec",
      "205dcf992ebb4234ae94732cba8e42e1",
      "facbba03e7d34ae980c7849c08467e2a",
      "a30ea8a01f6644309d3d222b5f724b04"
     ]
    },
    "id": "ZcDOumQVgY7g",
    "outputId": "518ed176-3271-44dc-cee7-a882dad0734a"
   },
   "outputs": [],
   "source": [
    "@interact(data_loader=fixed(train_dataloader), batch=IntSlider(min=0, max=len(train_dataloader)-1, step=1, value=0))\n",
    "def show_batch(data_loader, batch):\n",
    "    plt.rcParams['figure.figsize'] = [20, 15]\n",
    "\n",
    "    images, masks = list(itertools.islice(data_loader, batch, batch+1))[0]\n",
    "    masks_list = []\n",
    "    for image, mask in zip(images, masks):\n",
    "        masked = draw_segmentation_masks((image * 255).byte(),\n",
    "                                    mask.bool(), alpha=0.5, colors='red')\n",
    "        masks_list.append(masked)\n",
    "\n",
    "    grid  = make_grid(masks_list, nrow=6)\n",
    "    plt.imshow(grid.permute(1, 2, 0));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pzRe9rwYmhbG"
   },
   "source": [
    "# Composer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WN-jT-xwB4rh"
   },
   "source": [
    "## Model\n",
    "\n",
    "Here we define a composer model that wraps the smp [segmentation models pytorch](https://github.com/qubvel/segmentation_models.pytorch) package. This lets us quickly create many different segmentation models made from common pre-trained pytorch encoders. \n",
    "\n",
    "- We set defaults to create a [Unet](https://arxiv.org/abs/1505.04597) from an imagenet pretrained resnet34 with 3 input channels for our RGB (converted) inputs and 1 output channel. \n",
    "\n",
    "- We set the default loss to `nn.BCEWithLogitsLoss()` to classify each pixel of the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z-YGWK6KpGA2"
   },
   "outputs": [],
   "source": [
    "class SMPUNet(ComposerModel):\n",
    "\n",
    "    def __init__(self, encoder_name='resnet34', encoder_weights='imagenet',\n",
    "                 in_channels=3, classes=1, loss=nn.BCEWithLogitsLoss()):\n",
    "        super().__init__()\n",
    "        self.model = smp.Unet(encoder_name=encoder_name,\n",
    "                              encoder_weights=encoder_weights,     # use `imagenet` pre-trained weights for encoder initialization\n",
    "                              in_channels=in_channels,        # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "                              classes=classes)                # model output channels (number of classes in your dataset)\n",
    "\n",
    "        self.criterion = loss\n",
    "        self.train_loss = LossMetric(loss)\n",
    "        self.val_loss = LossMetric(loss)\n",
    "        self.val_dice = Dice(nclass=classes)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        images, targets = batch\n",
    "        return self.model(images)\n",
    "\n",
    "    def loss(self, outputs, batch):\n",
    "        _, targets = batch\n",
    "        return self.criterion(outputs, targets)\n",
    "\n",
    "    def metrics(self, train: bool = False):\n",
    "        return self.train_loss if self.train else MetricCollection([self.val_loss, self.dice])\n",
    "\n",
    "    def validate(self, batch):\n",
    "        images, targets = batch\n",
    "        return self.model(images), targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oaYcO6ImJ9qd"
   },
   "outputs": [],
   "source": [
    "model = SMPUNet() # define unet model\n",
    "optimizer = DecoupledAdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "km8xOI8VTQOh"
   },
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yQuRyoyaj73n"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model,\n",
    "                  train_dataloader=train_dataloader,\n",
    "                  eval_dataloader=eval_dataloader,\n",
    "                  max_duration=\"2ep\",\n",
    "                  optimizers=optimizer,\n",
    "                  device='gpu',\n",
    "                  precision='amp',\n",
    "                  seed=1337)\n",
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s46_temkP7Rw"
   },
   "source": [
    "\n",
    "## Methods\n",
    "- composer allows us to quickly experiment with algorithms that can speed up or improve the quality of our model. This is how we can add `CutOut` and `LabelSmoothing`\n",
    "\n",
    "- additionally, the composer trainer has builtin support for automatic mixed precision training and gradient accumulation to help train quickly and simulate larger batch sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VoXjk_K_4nd2"
   },
   "outputs": [],
   "source": [
    "from composer.algorithms import CutOut, LabelSmoothing\n",
    "\n",
    "model = SMPUNet() # define unet model\n",
    "optimizer = DecoupledAdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "algorithms = [CutOut(n_holes=1, length=10), LabelSmoothing(alpha=0.1)]\n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "                  train_dataloader=train_dataloader,\n",
    "                  eval_dataloader=eval_dataloader,\n",
    "                  max_duration=\"2ep\",\n",
    "                  optimizers=optimizer,\n",
    "                  device='gpu',\n",
    "                  precision='amp',\n",
    "                  seed=1337)\n",
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cdCDBS9mt71R"
   },
   "source": [
    "# Next steps\n",
    "- train longer\n",
    "- try different loss functions, architectures, transformations\n",
    "- try different combinations of composer methods!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "hShNYFabiNYn"
   ],
   "name": "Medical_Image_Segmentation_Composer.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0251f4a4e0f74fd788504e1360cb2dde": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "09ec07a854fc4b699da6a2533e2e105c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0251f4a4e0f74fd788504e1360cb2dde",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_69752d98aaea446d9636c53724b2d7e6",
      "value": 0
     }
    },
    "205dcf992ebb4234ae94732cba8e42e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "SliderStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "SliderStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": "",
      "handle_color": null
     }
    },
    "37c5e831be654cb8a50d704405fd86d3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "IntSliderModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntSliderModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "IntSliderView",
      "continuous_update": true,
      "description": "batch",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_facbba03e7d34ae980c7849c08467e2a",
      "max": 170,
      "min": 0,
      "orientation": "horizontal",
      "readout": true,
      "readout_format": "d",
      "step": 1,
      "style": "IPY_MODEL_205dcf992ebb4234ae94732cba8e42e1",
      "value": 0
     }
    },
    "3a95da793b424447a27ac5269e6d06c7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3bea6037e76146fdb31c158593089602": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3ff5c34ab24b4629938408c7e83ca184": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ce691b5958ca43beb35c7312125a9df7",
       "IPY_MODEL_09ec07a854fc4b699da6a2533e2e105c",
       "IPY_MODEL_c4e7c50f4724476d84c83fb71b520dc8"
      ],
      "layout": "IPY_MODEL_62afe8a7c8024e3c90b1cb65dbe05605"
     }
    },
    "62afe8a7c8024e3c90b1cb65dbe05605": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "69752d98aaea446d9636c53724b2d7e6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7c8c64e66821442a8ca73dd47ea45235": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8f8443cdd1834bf4a1e0ed890c7af6e6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a1b55972432441d8870363e953ffd7e3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [
       "widget-interact"
      ],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_37c5e831be654cb8a50d704405fd86d3",
       "IPY_MODEL_89c0a9217cd7467bacdef3534acfbfec"
      ],
      "layout": "IPY_MODEL_3a95da793b424447a27ac5269e6d06c7"
     }
    },
    "a30ea8a01f6644309d3d222b5f724b04": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c4e7c50f4724476d84c83fb71b520dc8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3bea6037e76146fdb31c158593089602",
      "placeholder": "​",
      "style": "IPY_MODEL_8f8443cdd1834bf4a1e0ed890c7af6e6",
      "value": " 0/0 [00:00&lt;?, ?it/s]"
     }
    },
    "cac458165c834216a454a3e3e709e1d0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ce691b5958ca43beb35c7312125a9df7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7c8c64e66821442a8ca73dd47ea45235",
      "placeholder": "​",
      "style": "IPY_MODEL_cac458165c834216a454a3e3e709e1d0",
      "value": ""
     }
    },
    "facbba03e7d34ae980c7849c08467e2a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
