{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Composer Functional.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMPh49wtQb7H"
      },
      "outputs": [],
      "source": [
        "# install composer, hiding output to keep the notebook clean\n",
        "# ! pip install mosaicml > /dev/null 2>&1\n",
        "# To install from source instead of the last release, use this command instead:|\n",
        "! pip install git+https://github.com/mosaicml/composer.git@dev > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using the Functional API\n",
        "\n",
        "In this tutorial, we'll see an example of using Composer's algorithms in a standalone fashion, with no changes to the surrounding code and no requirement to use the Composer trainer. We'll be training a simple model on CIFAR-10, similar to the [PyTorch classifier tutorial](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html). Because we'll be using a toy model trained for only a few epochs, we won't get the same speed or accuracy gains we might expect from a more realistic problem. However, this notebook should still serve as a useful illustration of how to use various algorithms. For examples of more realistic results, see the MosaicML [Explorer](https://app.mosaicml.com/explorer/imagenet).\n",
        "\n",
        "First, we need to define our original model, dataloader, and training loop. Let's start with the dataloader:"
      ],
      "metadata": {
        "id": "DSzcOUUDOkow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "datadir = './data'\n",
        "batch_size = 1024\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root=datadir, train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root=datadir, train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "BchTXTUCRPXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's define our model. We're going to use a toy convolutional neural network so that the training epochs finish quickly."
      ],
      "metadata": {
        "id": "FCZYCbaZ8JeW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=(3, 3), stride=2)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=(3, 3))\n",
        "        self.norm = nn.BatchNorm2d(32)\n",
        "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc1 = nn.Linear(32, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(self.norm(x))\n",
        "        x = torch.flatten(self.pool(x), 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "7FszuZXV3Hn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, let's write a simple training loop that prints the accuracy on the test set at the end of each epoch. We'll just run a few epochs for brevity."
      ],
      "metadata": {
        "id": "a4xSC20f8WdG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm\n",
        "from composer import functional as cf\n",
        "\n",
        "num_epochs = 5\n",
        "\n",
        "def train_and_eval(model, train_loader, test_loader, device='cuda'):\n",
        "    torch.manual_seed(42)\n",
        "    model = model.to(device)\n",
        "    opt = torch.optim.Adam(model.parameters())\n",
        "    for epoch in range(num_epochs):\n",
        "        print(\"---- Begginning epoch \", epoch)\n",
        "        model.train()\n",
        "        progress_bar = tqdm(train_loader)\n",
        "        for X, y in progress_bar:\n",
        "            X = X.to(device)\n",
        "            y_hat = model(X).to('cpu')\n",
        "            loss = F.cross_entropy(y_hat, y)\n",
        "            progress_bar.set_postfix_str(\"train loss: {:.4f}\".format(loss.detach().numpy()))\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            opt.zero_grad()\n",
        "        model.eval()\n",
        "        num_right = 0\n",
        "        eval_size = 0\n",
        "        for X, y in test_loader:\n",
        "            y_hat = model(X.to(device)).to('cpu')\n",
        "            num_right += (y_hat.argmax(dim=1) == y).sum().numpy()\n",
        "            eval_size += len(y)\n",
        "        acc_percent = 100 * num_right / eval_size\n",
        "        print(\"Epoch {} validation accuracy: {:.2f}%\".format(epoch, acc_percent))"
      ],
      "metadata": {
        "id": "BdKT93zI72PR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great. Now let's instantiate this baseline model and see how it fares on our dataset.\n"
      ],
      "metadata": {
        "id": "21IVgPpd7oNC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net()\n",
        "train_and_eval(model, trainloader, testloader)"
      ],
      "metadata": {
        "id": "FQ_iwrxR7m3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have this baseline, let's add algorithms to improve our data pipeline and model. We'll start by adding some data augmentation."
      ],
      "metadata": {
        "id": "4mZrNRMSymFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Sequence\n",
        "import composer.functional as cf\n",
        "import functools\n",
        "import sys\n",
        "import composer\n",
        "\n",
        "# create dataloaders for the train and test sets\n",
        "shared_transforms = [transforms.ToTensor(),\n",
        "  transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        "\n",
        "train_transforms = shared_transforms[:] + [cf.colout_batch]\n",
        "\n",
        "test_transform = transforms.Compose(shared_transforms)\n",
        "train_transform = transforms.Compose(train_transforms)\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root=datadir, train=True,\n",
        "                                        download=True, transform=train_transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                        shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root=datadir, train=False,\n",
        "                                        download=True, transform=test_transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                          shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "udTtW9JUmtKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how our model does with just these changes."
      ],
      "metadata": {
        "id": "GA6eLT7w9bBe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net()\n",
        "# only use one data augmentation since our small model runs quickly\n",
        "# and allows the dataloader little time to do anything fancy\n",
        "train_and_eval(model, trainloader, testloader)"
      ],
      "metadata": {
        "id": "oXfFDF_VUTY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we might expect, adding data augmentation doesn't help us when we aren't training long enough to start overfitting.\n",
        "\n",
        "Now let's try using some algorithms that modify the model. We're going to keep things simple and just add a [Squeeze-and-Excitation](https://docs.mosaicml.com/en/latest/method_cards/squeeze_excite.html) module after the larger of the two conv2d operations in our model. We show examples of how to use other algorithms as well as a reference."
      ],
      "metadata": {
        "id": "TQ3pd7LvUTAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# squeeze-excite can add a lot of overhead for small\n",
        "# conv2d operations, so only add it after convs with a\n",
        "# minimum number of channels\n",
        "cf.apply_squeeze_excite(model, latent_channels=64, min_channels=16)\n",
        "cf.apply_blurpool(model)"
      ],
      "metadata": {
        "id": "8bjbKUwFvy9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's see how our model does with all of the above algorithms applied."
      ],
      "metadata": {
        "id": "-5oyzjhy6d-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_eval(model, trainloader, testloader)"
      ],
      "metadata": {
        "id": "LxxwlU4S6ZvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding squeeze-excite and blurpool gives us another few percentage points of accuracy, and does so with little decrease in the iterations per second. Great!\n",
        "\n",
        "Of course, this is a toy model and an unrealistically short training duration, so almost nothing should be read into these results. But hopefully this served as a useful illustration of how to use Composer's algorithms without the Composer trainer, and with minimal change to your code. If you hit any problems or have questions, feel free to [open an issue](https://github.com/mosaicml/composer/issues/new/) or reach out to us [on Slack](https://join.slack.com/t/mosaicml-community/shared_invite/zt-w0tiddn9-WGTlRpfjcO9J5jyrMub1dg)."
      ],
      "metadata": {
        "id": "OwHNCGvUPNwO"
      }
    }
  ]
}