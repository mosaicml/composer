{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook will walk you through how to accelerate model training with Composer. We'll start by training a baseline ResNet56 on CIFAR10, then see how training efficiency improves as we add speed-up methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Composer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by installing composer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/mosaicml/composer.git@dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up Our Workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we'll set up our workspace. We'll import the necessary packages, and setup our dataset and trainer. First, the imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "\n",
    "import composer\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "torch.manual_seed(42) # For replicability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we instantiate our CIFAR10 dataset and dataloader. Composer has it's own CIFAR10 dataset and dataloaders, but this walkthrough focuses on how to use Composer's algorithms, so we'll stick with the Torchvision CIFAR10 and PyTorch dataloader for the sake of familiarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = \"../data\"\n",
    "\n",
    "# Normalization constants\n",
    "mean = (0.507, 0.487, 0.441)\n",
    "std = (0.267, 0.256, 0.276)\n",
    "\n",
    "batch_size = 1024\n",
    "\n",
    "cifar10_transforms = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean, std)])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(data_directory, train=True, download=True, transform=cifar10_transforms)\n",
    "test_dataset = datasets.CIFAR10(data_directory, train=False, download=True, transform=cifar10_transforms)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create our model. We're using composer's built-in ResNet56. To use your own custom model, please see the [custom models tutorial](https://docs.mosaicml.com/en/v0.3.1/tutorials/adding_models_datasets.html#models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from composer import models\n",
    "model = models.CIFAR10_ResNet56()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trainer will handle instantiating the optimizer, but first we need to create the optimizer and LR scheduler. We're using [MosaicML's SGD with decoupled weight decay](https://arxiv.org/abs/1711.05101):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = composer.optim.DecoupledSGDW(\n",
    "    model.parameters(), # Model parameters to update\n",
    "    lr=0.05, # Peak learning rate\n",
    "    momentum=0.9,\n",
    "    weight_decay=2.0e-3 # If this looks large, it's because its not scaled by the LR as in non-decoupled weight decay\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll assume this is being run on Colab, which means training for hundreds of epochs would take a very long time. Instead we'll train our baseline model for three epochs. The first epoch will be linear warmup. We achieve this by instantiating a `WarmUpLRHparams` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Having to manually compute the number of iterations is ungainly\n",
    "\n",
    "warmup = composer.optim.WarmUpLR(\n",
    "    optimizer, # Optimizer\n",
    "    warmup_iters=25, # Number of iterations to warmup over. 50k samples * 1 batch/2048 samples\n",
    "    warmup_method=\"linear\", # Linear warmup\n",
    "    warmup_factor=1e-4, # Initial LR = LR * warmup_factor\n",
    "    interval=\"step\", # Update LR with stepwise granularity for superior results\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also use cosine decay for the next two epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Seems like there's no way to have stepwise resolution for cosine decay without using CosineAnnealingLRHparams :(\n",
    "\n",
    "# decay = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "#     optimizer,\n",
    "#     T_max=50 # 2 epochs == 50 steps\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An LR schedule is provided to the trainer as a list of SchedulerHparams objects. In this case we just have one, but creating a more complex LR schedule is as simple as collecting the SchedulerHparams objects in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule = [warmup] # Complex LR schedules achieved by [warmup_scheduler_object, decay_scheduler_object, ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we create our trainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Need in-memory logging to store and plot results\n",
    "\n",
    "train_epochs = \"3ep\" # Train for 3 epochs because we're assuming Colab environment and hardware\n",
    "device = \"gpu\" # Train on the GPU\n",
    "\n",
    "trainer = composer.trainer.Trainer(\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    eval_dataloader=test_dataloader,\n",
    "    max_duration=train_epochs,\n",
    "    optimizers=optimizer,\n",
    "    schedulers=lr_schedule,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Would be nice to be able to plot something here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train and measure the training time below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "trainer_fit"
    ]
   },
   "outputs": [],
   "source": [
    "start_time = time.perf_counter()\n",
    "trainer.fit()\n",
    "end_time = time.perf_counter()\n",
    "print(f\"It took {end_time - start_time:0.4f} seconds to train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're running this on Colab, the runtime will vary a lot based on the instance. We found that the three epochs of training could take anywhere from 120-550 seconds to run, and the mean validation accuracy was typically in the range of 25%-40%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Algorithms to Speed Up Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the things we're most excited about at MosaicML is our speed-up algorithms. We used these algorithms to [speed up training of ResNet50 on ImageNet by up to 3.4x](https://app.mosaicml.com/explorer/imagenet). Let's try applying a few algorithms to make our ResNet56 more efficient.\n",
    "\n",
    "We'll start with [ColOut](https://docs.mosaicml.com/en/v0.3.1/method_cards/col_out.html), which is an in-house invention. Colout drops rows and columns of an image with probability *p*. It's a little bit like [Random Erasing](https://arxiv.org/abs/1708.04896) except it reduces the size of the image, which can increase data throughput and speed up training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colout = composer.algorithms.ColOut()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also use [BlurPool](https://docs.mosaicml.com/en/v0.3.1/method_cards/blurpool.html), which increases accuracy by applying a spatial low-pass filter before the pool in max pooling and whenever using a strided convolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blurpool = composer.algorithms.BlurPool(\n",
    "    replace_convs=True, # Blur before convs\n",
    "    replace_maxpools=True, # Blur before max-pools\n",
    "    blur_first=True # Blur before conv/max-pool\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final algorithm in our improved training recipe is [Progressive Image Resizing](https://docs.mosaicml.com/en/v0.3.1/method_cards/progressive_resizing_vision.html). Progressive Image Resizing initially shrinks the size of training images and slowly scales them back to their full size over the course of training. It increases throughput during the early phase of training, when the network may learn coarse-grained features that do not require details lost by reducing image resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prog_resize = composer.algorithms.ProgressiveResizing(\n",
    "    initial_scale=.6, # Size of images at the beginning of training = .6 * default image size\n",
    "    finetune_fraction=0.33 # Train on default size images for 0.5 of total training time.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll assemble all our algorithms into a list to pass to our trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = [colout, blurpool, prog_resize]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's instantiate our model, optimizer, scheduler, and trainer again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.CIFAR10_ResNet56()\n",
    "\n",
    "optimizer = composer.optim.DecoupledSGDW(\n",
    "    model.parameters(),\n",
    "    lr=0.05,\n",
    "    momentum=0.9,\n",
    "    weight_decay=2.0e-3\n",
    "    )\n",
    "\n",
    "warmup = composer.optim.WarmUpLR(\n",
    "    optimizer, # Optimizer\n",
    "    warmup_iters=25, # Number of iterations to warmup over. 50k samples * 1 batch/2048 samples\n",
    "    warmup_method=\"linear\", # Linear warmup\n",
    "    warmup_factor=1e-4, # Initial LR = LR * warmup_factor\n",
    "    interval=\"step\", # Update LR with stepwise granularity for superior results\n",
    ")\n",
    "\n",
    "lr_schedule = [warmup]\n",
    "\n",
    "trainer = composer.trainer.Trainer(\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    eval_dataloader=test_dataloader,\n",
    "    max_duration=train_epochs,\n",
    "    optimizers=optimizer,\n",
    "    schedulers=lr_schedule,\n",
    "    device=device,\n",
    "    algorithms=algorithms # Adding algorithms this time\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's get training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "trainer_fit"
    ]
   },
   "outputs": [],
   "source": [
    "start_time = time.perf_counter()\n",
    "trainer.fit()\n",
    "end_time = time.perf_counter()\n",
    "print(f\"It took {end_time - start_time:0.4f} seconds to train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the runtime will vary based on the instance, but we found that it took about a 0.43x-0.75x as long to train (a 1.3x-2.3x speedup, which corresponds to 90-400 seconds) relative to the baseline recipe without augmentations. We also found that validation accuracy was similar for the algorithm-enhanced and baseline recipes.\n",
    "\n",
    "Because ColOut and Progressive Resizing increase data throughput (i.e. more samples per sceond), we can train for more iterations in the same amount of wall clock time. Let's train for another epoch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_epochs = \"1ep\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to set up our optimizer, scheduler, and trainer again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "trainer_fit"
    ]
   },
   "outputs": [],
   "source": [
    "optimizer = composer.optim.DecoupledSGDW(\n",
    "    model.parameters(),\n",
    "    lr=0.05,\n",
    "    momentum=0.9,\n",
    "    weight_decay=2.0e-3\n",
    "    )\n",
    "\n",
    "# The Composer trainer defaults to cosine decay if no schedule is passed to it,\n",
    "# so let's creata constant LR schedule\n",
    "flat_schedule = composer.optim.ConstantLR(\n",
    "    optimizer\n",
    ")\n",
    "\n",
    "lr_schedule = [flat_schedule]\n",
    "\n",
    "trainer = composer.trainer.Trainer(\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    eval_dataloader=test_dataloader,\n",
    "    max_duration='5ba',\n",
    "    optimizers=optimizer,\n",
    "    schedulers=lr_schedule,\n",
    "    device=device,\n",
    "    algorithms=[colout, blurpool] # Adding algorithms this time\n",
    ")\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "trainer.fit()\n",
    "end_time = time.perf_counter()\n",
    "print(f\"It took {end_time - start_time:0.4f} seconds to train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found that using these speed-up algorithms for four epochs resulted in runtime similar to or less than three epochs *without* speed-up algorithms (120-550 seconds, depending on the instance), and that they usually improved validation accuracy by 5-15 percentage points, yielding validation accuracy in the range of 30%-50%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You did it! Now come get involved with MosaicML!\n",
    "\n",
    "Hopefully you're now comfortable with the basics of training with Composer. We'd love for you to get involved with MosaicML community in any of these ways:\n",
    "\n",
    "## [Star Composer on GitHub](https://github.com/mosaicml/composer)\n",
    "\n",
    "Stay up-to-date and help make others aware of our work by [starring Composer on GitHub](https://github.com/mosaicml/composer).\n",
    "\n",
    "## [Join the MosaicML Slack](https://join.slack.com/t/mosaicml-community/shared_invite/zt-w0tiddn9-WGTlRpfjcO9J5jyrMub1dg)\n",
    "\n",
    "Head on over to the [MosaicML slack](https://join.slack.com/t/mosaicml-community/shared_invite/zt-w0tiddn9-WGTlRpfjcO9J5jyrMub1dg) to join other ML efficiency enthusiasts. Come for the paper discussions, stay for the memes!\n",
    "\n",
    "## Contribute to Composer\n",
    "\n",
    "Is there a bug you noticed or a feature you'd like? File an [issue](https://github.com/mosaicml/composer/issues) or make [pull request](https://github.com/mosaicml/composer/pulls)!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Fast Training of ResNet56 on CIFAR10.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
