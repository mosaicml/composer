{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUslqRhZbp-P"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "This notebook will walk you through how to accelerate model training with Composer. We'll start by training a baseline ResNet56 on CIFAR10, then see how training efficiency improves as we add speed-up methods.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dymD54IWbp-R"
      },
      "source": [
        "# Install Composer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbQk7oR9bp-R"
      },
      "source": [
        "We'll start by installing composer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwMMm8-Ubp-R"
      },
      "outputs": [],
      "source": [
        "!pip install mosaicml "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfZXHsxDbp-S"
      },
      "source": [
        "# Set Up Our Workspace"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gH1xK-t7bp-S"
      },
      "source": [
        "## Imports\n",
        "\n",
        "In this section we'll set up our workspace. We'll import the necessary packages, and setup our dataset and trainer. First, the imports:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BcIPe7rvbp-S"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "import torch\n",
        "\n",
        "import composer\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "torch.manual_seed(42) # For replicability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_-vWQtZbp-S"
      },
      "source": [
        "## Dataset & DataLoader\n",
        "\n",
        "Next, we instantiate our CIFAR10 dataset and dataloader. Composer has it's own CIFAR10 dataset and dataloaders, but this walkthrough focuses on how to use Composer's algorithms, so we'll stick with the Torchvision CIFAR10 and PyTorch dataloader for the sake of familiarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQzMsA8lbp-T"
      },
      "outputs": [],
      "source": [
        "data_directory = \"../data\"\n",
        "\n",
        "# Normalization constants\n",
        "mean = (0.507, 0.487, 0.441)\n",
        "std = (0.267, 0.256, 0.276)\n",
        "\n",
        "batch_size = 1024\n",
        "\n",
        "cifar10_transforms = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean, std)])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(data_directory, train=True, download=True, transform=cifar10_transforms)\n",
        "test_dataset = datasets.CIFAR10(data_directory, train=False, download=True, transform=cifar10_transforms)\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVvmfEKdbp-T"
      },
      "source": [
        "## Model\n",
        "\n",
        "Next, we create our model. We're using composer's built-in ResNet56. To use your own custom model, please see the [custom models tutorial](https://docs.mosaicml.com/en/stable/tutorials/adding_models_datasets.html#models)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hTfYto1bp-T"
      },
      "outputs": [],
      "source": [
        "from composer import models\n",
        "model = models.CIFAR10_ResNet56()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbF3v_Wtbp-T"
      },
      "source": [
        "## Optimizer and Scheduler\n",
        "\n",
        "The trainer will handle instantiating the optimizer, but first we need to create the optimizer and LR scheduler. We're using [MosaicML's SGD with decoupled weight decay](https://arxiv.org/abs/1711.05101):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_KwMpr8bp-T"
      },
      "outputs": [],
      "source": [
        "optimizer = composer.optim.DecoupledSGDW(\n",
        "    model.parameters(), # Model parameters to update\n",
        "    lr=0.05, # Peak learning rate\n",
        "    momentum=0.9,\n",
        "    weight_decay=2.0e-3 # If this looks large, it's because its not scaled by the LR as in non-decoupled weight decay\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoyO-A_3bp-U"
      },
      "source": [
        "We'll assume this is being run on Colab, which means training for hundreds of epochs would take a very long time. Instead we'll train our baseline model for three epochs. The first epoch will be linear warmup, followed by two epochs of constant LR. We achieve this by instantiating a `LinearWithWarmupScheduler` class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LA8t-1Jhbp-U"
      },
      "outputs": [],
      "source": [
        "lr_scheduler = composer.optim.LinearWithWarmupScheduler(\n",
        "    t_warmup=\"1ep\", # Warm up over 1 epoch\n",
        "    alpha_i=1.0, # Flat LR schedule achieved by having alpha_i == alpha_f\n",
        "    alpha_f=1.0\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nq1Y_cy8bp-U"
      },
      "source": [
        "# Train a Baseline Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQNxs10Abp-U"
      },
      "source": [
        "And now we create our trainer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7WIznFcUbp-U"
      },
      "outputs": [],
      "source": [
        "train_epochs = \"3ep\" # Train for 3 epochs because we're assuming Colab environment and hardware\n",
        "device = \"gpu\" # Train on the GPU\n",
        "\n",
        "trainer = composer.trainer.Trainer(\n",
        "    model=model,\n",
        "    train_dataloader=train_dataloader,\n",
        "    eval_dataloader=test_dataloader,\n",
        "    max_duration=train_epochs,\n",
        "    optimizers=optimizer,\n",
        "    schedulers=lr_scheduler,\n",
        "    device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrOH4-5Ibp-V"
      },
      "source": [
        "We train and measure the training time below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBUn-p9Nbp-V"
      },
      "outputs": [],
      "source": [
        "start_time = time.perf_counter()\n",
        "trainer.fit()\n",
        "end_time = time.perf_counter()\n",
        "print(f\"It took {end_time - start_time:0.4f} seconds to train\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxXpfAoLbp-V"
      },
      "source": [
        "If you're running this on Colab, the runtime will vary a lot based on the instance. We found that the three epochs of training could take anywhere from 120-550 seconds to run, and the mean validation accuracy was typically in the range of 25%-40%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCSkh4XQbp-V"
      },
      "source": [
        "# Use Algorithms to Speed Up Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atF2RZZsbp-V"
      },
      "source": [
        "One of the things we're most excited about at MosaicML is our speed-up algorithms. We used these algorithms to [speed up training of ResNet50 on ImageNet by up to 3.4x](https://app.mosaicml.com/explorer/imagenet). Let's try applying a few algorithms to make our ResNet56 more efficient.\n",
        "\n",
        "We'll start with [ColOut](https://docs.mosaicml.com/en/stable/method_cards/col_out.html), which is an in-house invention. Colout drops rows and columns of an image with probability *p*. It's a little bit like [Random Erasing](https://arxiv.org/abs/1708.04896) except it reduces the size of the image, which can increase data throughput and speed up training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jiy3D98vbp-W"
      },
      "outputs": [],
      "source": [
        "colout = composer.algorithms.ColOut()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcnnZDxSbp-W"
      },
      "source": [
        "Let's also use [BlurPool](https://docs.mosaicml.com/en/stable/method_cards/blurpool.html), which increases accuracy by applying a spatial low-pass filter before the pool in max pooling and whenever using a strided convolution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_m54MZB3bp-W"
      },
      "outputs": [],
      "source": [
        "blurpool = composer.algorithms.BlurPool(\n",
        "    replace_convs=True, # Blur before convs\n",
        "    replace_maxpools=True, # Blur before max-pools\n",
        "    blur_first=True # Blur before conv/max-pool\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CE5G9NBRbp-W"
      },
      "source": [
        "Our final algorithm in our improved training recipe is [Progressive Image Resizing](https://docs.mosaicml.com/en/stable/method_cards/progressive_resizing_vision.html). Progressive Image Resizing initially shrinks the size of training images and slowly scales them back to their full size over the course of training. It increases throughput during the early phase of training, when the network may learn coarse-grained features that do not require details lost by reducing image resolution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAURRwsAbp-W"
      },
      "outputs": [],
      "source": [
        "prog_resize = composer.algorithms.ProgressiveResizing(\n",
        "    initial_scale=.6, # Size of images at the beginning of training = .6 * default image size\n",
        "    finetune_fraction=0.34, # Train on default size images for 0.34 of total training time.\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WN-Cd5qtbp-X"
      },
      "source": [
        "We'll assemble all our algorithms into a list to pass to our trainer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_zZ2mmbbp-X"
      },
      "outputs": [],
      "source": [
        "algorithms = [colout, blurpool, prog_resize]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rn1Tcuu-bp-X"
      },
      "source": [
        "Now let's instantiate our model, optimizer, and trainer again. No need to instantiate our scheduler again because it's stateless!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qITr1xgYbp-X"
      },
      "outputs": [],
      "source": [
        "model = models.CIFAR10_ResNet56()\n",
        "\n",
        "optimizer = composer.optim.DecoupledSGDW(\n",
        "    model.parameters(),\n",
        "    lr=0.05,\n",
        "    momentum=0.9,\n",
        "    weight_decay=2.0e-3\n",
        "    )\n",
        "\n",
        "trainer = composer.trainer.Trainer(\n",
        "    model=model,\n",
        "    train_dataloader=train_dataloader,\n",
        "    eval_dataloader=test_dataloader,\n",
        "    max_duration=train_epochs,\n",
        "    optimizers=optimizer,\n",
        "    schedulers=lr_scheduler,\n",
        "    device=device,\n",
        "    algorithms=algorithms # Adding algorithms this time\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJrhOghFbp-X"
      },
      "source": [
        "And let's get training!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zH4ru0bKbp-Y"
      },
      "outputs": [],
      "source": [
        "start_time = time.perf_counter()\n",
        "trainer.fit()\n",
        "end_time = time.perf_counter()\n",
        "three_epochs_accelerated_time = end_time - start_time\n",
        "print(f\"It took {three_epochs_accelerated_time:0.4f} seconds to train\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f28pOQ14bp-Y"
      },
      "source": [
        "Again, the runtime will vary based on the instance, but we found that it took about a 0.43x-0.75x as long to train (a 1.3x-2.3x speedup, which corresponds to 90-400 seconds) relative to the baseline recipe without augmentations. We also found that validation accuracy was similar for the algorithm-enhanced and baseline recipes.\n",
        "\n",
        "Because ColOut and Progressive Resizing increase data throughput (i.e. more samples per sceond), we can train for more iterations in the same amount of wall clock time. Let's train our model for one additional epoch!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gw1YDURobp-Y"
      },
      "outputs": [],
      "source": [
        "train_epochs = \"1ep\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FJd2SM3bp-Y"
      },
      "source": [
        "Resuming training means we'll need to use a flat LR schedule:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjnwtq9Fbp-Y"
      },
      "outputs": [],
      "source": [
        "lr_scheduler = composer.optim.scheduler.ConstantScheduler(alpha=1.0, t_max='1dur')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXyJ0zSebp-Y"
      },
      "source": [
        "And we can also get rid of progressive resizing (because we want to train on the full size images for this additional epoch), and the model already has blurpool enabled, so we don't need to pass that either:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCHexGrHbp-Y"
      },
      "outputs": [],
      "source": [
        "algorithms = [colout]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4f0caiKYbp-Y"
      },
      "outputs": [],
      "source": [
        "trainer = composer.trainer.Trainer(\n",
        "    model=model,\n",
        "    train_dataloader=train_dataloader,\n",
        "    eval_dataloader=test_dataloader,\n",
        "    max_duration=train_epochs,\n",
        "    optimizers=optimizer,\n",
        "    schedulers=lr_scheduler,\n",
        "    device=device,\n",
        "    algorithms=algorithms\n",
        ")\n",
        "\n",
        "start_time = time.perf_counter()\n",
        "trainer.fit()\n",
        "\n",
        "end_time = time.perf_counter()\n",
        "final_epoch_accelerated_time = end_time - start_time\n",
        "# Time for four epochs = time for three epochs + time for fourth epoch\n",
        "four_epochs_accelerated_time = three_epochs_accelerated_time + final_epoch_accelerated_time\n",
        "print(f\"It took {four_epochs_accelerated_time:0.4f} seconds to train\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntJIphzxbp-Z"
      },
      "source": [
        "We found that using these speed-up algorithms for four epochs resulted in runtime similar to or less than three epochs *without* speed-up algorithms (120-550 seconds, depending on the instance), and that they usually improved validation accuracy by 5-15 percentage points, yielding validation accuracy in the range of 30%-50%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5EBaX0wbp-Z"
      },
      "source": [
        "# You did it! Now come get involved with MosaicML!\n",
        "\n",
        "Hopefully you're now comfortable with the basics of training with Composer. We'd love for you to get involved with MosaicML community in any of these ways:\n",
        "\n",
        "## [Star Composer on GitHub](https://github.com/mosaicml/composer)\n",
        "\n",
        "Stay up-to-date and help make others aware of our work by [starring Composer on GitHub](https://github.com/mosaicml/composer).\n",
        "\n",
        "## [Join the MosaicML Slack](https://join.slack.com/t/mosaicml-community/shared_invite/zt-w0tiddn9-WGTlRpfjcO9J5jyrMub1dg)\n",
        "\n",
        "Head on over to the [MosaicML slack](https://join.slack.com/t/mosaicml-community/shared_invite/zt-w0tiddn9-WGTlRpfjcO9J5jyrMub1dg) to join other ML efficiency enthusiasts. Come for the paper discussions, stay for the memes!\n",
        "\n",
        "## Contribute to Composer\n",
        "\n",
        "Is there a bug you noticed or a feature you'd like? File an [issue](https://github.com/mosaicml/composer/issues) or make [pull request](https://github.com/mosaicml/composer/pulls)!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "up_and_running_with_composer.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
