{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install composer\n",
    "# To install from source instead of the last release, use the below command instead:\n",
    "!pip install mosaicml[nlp] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial will demonstrate how to fine-tune a pretrained HuggingFace transformer using the composer library! Composer provides a highly optimized and functional training loop and the ability to compose several methods that can accelerate training.\n",
    "\n",
    "We will focus on fine-tuning a pretrained BERT-base model on the Stanford Sentiment Treebank v2 (SST-2) dataset. After fine-tuning, the BERT model should be able to determine if a setence has positive or negative sentiment.\n",
    "\n",
    "Let's do this ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining a Composer Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first task is to create a composer model. A composer model defines four components for the composer trainer:\n",
    "- `forward()` - parses the dataloader output for the model's forward function and extracts the necessary components of the model's output for loss calculation.\n",
    "- `loss()` - computes the loss for the current batch using the model and dataloader outputs.\n",
    "- `validate()` - parses the dataloader and model output for the torchmetrics.\n",
    "- `metrics()` - defines the torchmetrics to use during training/validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from torchmetrics import Accuracy\n",
    "from torchmetrics.collections import MetricCollection\n",
    "from composer.models.base import ComposerModel\n",
    "from composer.models.nlp_metrics import LanguageCrossEntropyLoss\n",
    "\n",
    "# Define a Composer Model\n",
    "class ComposerBERT(ComposerModel):\n",
    "  def __init__(self, model):\n",
    "    super().__init__()\n",
    "    self.module = model\n",
    "\n",
    "    # Metrics\n",
    "    self.train_loss = LanguageCrossEntropyLoss()\n",
    "    self.val_loss = LanguageCrossEntropyLoss()\n",
    "\n",
    "    self.train_acc = Accuracy()\n",
    "    self.val_acc = Accuracy()\n",
    "\n",
    "  def forward(self, batch):\n",
    "    output = self.module(**batch)\n",
    "    return output\n",
    "\n",
    "  def loss(self, outputs, batch):\n",
    "    return outputs['loss']\n",
    "\n",
    "  def validate(self, batch):\n",
    "    labels = batch.pop('labels')\n",
    "    output = self.forward(batch)\n",
    "    output = output['logits']\n",
    "\n",
    "    return (output, labels)\n",
    "  \n",
    "  def metrics(self, train: bool = False):\n",
    "    return MetricCollection([self.train_loss, self.train_acc]) if train else MetricCollection([self.val_loss, self.val_acc])\n",
    "\n",
    "# Create a BERT sequence classification model using HuggingFace transformsers\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2) # in BERT hparams\n",
    "\n",
    "# Package as a composer model\n",
    "composer_model = ComposerBERT(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will download and tokenize the SST-2 datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "# Create BERT tokenizer\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained('bert-base-uncased') # from transformer_shared\n",
    "def tokenize_function(sample):\n",
    "  return tokenizer(text=sample['sentence'], padding=\"max_length\", max_length=256, truncation=True)\n",
    "\n",
    "# Tokenize SST-2\n",
    "sst2_dataset = datasets.load_dataset(\"glue\", \"sst2\")\n",
    "tokenized_sst2_dataset = sst2_dataset.map(tokenize_function,\n",
    "                                          batched=True, \n",
    "                                          num_proc=cpu_count(),\n",
    "                                          batch_size=1000,\n",
    "                                          remove_columns=['idx', 'sentence'])\n",
    "\n",
    "# Split dataset into train and validation sets\n",
    "train_dataset = tokenized_sst2_dataset[\"train\"]\n",
    "eval_dataset = tokenized_sst2_dataset[\"validation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will create a PyTorch `Dataloader` for each of the datasets generated in the previous block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "data_collator = transformers.data.data_collator.default_data_collator\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=False, drop_last=False, collate_fn=data_collator)\n",
    "eval_dataloader = DataLoader(eval_dataset,batch_size=16, shuffle=False, drop_last=False, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the composer `Trainer`, we need to define a `split_batch` function. This function defines how to split the dataloader output into several \"microbatches\". Microbatchs are chunks of the batch that were divided based on the amount of gradient accumulation used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from composer.core import DataSpec\n",
    "\n",
    "def split_batch_dict(batch, n_microbatches: int):\n",
    "      chunked = {k: v.chunk(n_microbatches) for k, v in batch.items()}\n",
    "      num_chunks = len(list(chunked.values())[0])\n",
    "      return [{k: v[idx] for k, v in chunked.items()} for idx in range(num_chunks)]\n",
    "\n",
    "train_dataspec = DataSpec(dataloader=train_dataloader,\n",
    "                          split_batch=split_batch_dict)\n",
    "eval_dataspec = DataSpec(dataloader=eval_dataloader,\n",
    "                         split_batch=split_batch_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers and Learning Rate Schedulers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last setup step is to create an optimizer and a learning rate scheduler. We will use PyTorch's AdamW optimizer and linear learning rate scheduler since these are typically used to fine-tune BERT on tasks such as SST-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LinearLR\n",
    "\n",
    "optimizer = AdamW(params=composer_model.parameters(), lr=3e-5, betas=(0.9, 0.98), eps=1e-6, weight_decay=3e-6)\n",
    "linear_lr_decay = LinearLR(optimizer, start_factor=1.0, end_factor=0, total_iters=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Composer Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now specify a composer `Trainer` object and run our training! `Trainer` has many arguments that are described in our [documentation](https://docs.mosaicml.com/en/stable/trainer.html#composer.Trainer), but let's discuss the less obvious arguments used below:\n",
    "- `max_duration` - a string specifying how long to train either in terms of batches (e.g. '10ba' is 10 batches) or epochs (e.g. '1ep' is 1 epoch).\n",
    "- `schedulers` - a list of PyTorch learning rate schedulers that will be composed together.\n",
    "- `device` - specifies if the training will be done on CPU or GPU by using 'cpu' or 'gpu', respectively.\n",
    "- `train_subset_num_batches` - specifies the number of training batches to use for each epoch. This is not a necessary argument, but is useful for quickly testing code.\n",
    "- `precision` - whether to do the training in full precision 'fp32' or mixed precision 'amp'. Mixed precision provides almost 2x speedup in training time on certain hardware. If you get a P100, try `precision='amp'`!\n",
    "- `seed` - sets the random seed for the training run, so the results are reproducible!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from composer import Trainer\n",
    "\n",
    "# Create Trainer Object\n",
    "trainer = Trainer(model=composer_model, \n",
    "                  train_dataloader=train_dataspec,\n",
    "                  eval_dataloader=eval_dataspec,\n",
    "                  max_duration=\"1ep\",\n",
    "                  optimizers=optimizer,\n",
    "                  schedulers=[linear_lr_decay],\n",
    "                  device='gpu',\n",
    "                  train_subset_num_batches=150,\n",
    "                  precision='fp32',\n",
    "                  seed=17\n",
    "                  )\n",
    "# Start training\n",
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model reaches almost 86% accuracy with only 100 iterations of training! Let's visualize a few samples from the validation set to see how our model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "eval_batch = next(iter(eval_dataloader))\n",
    "\n",
    "# Move batch to gpu\n",
    "eval_batch = {k: v.cuda() for k, v in eval_batch.items()}\n",
    "with torch.no_grad():\n",
    "  predictions = composer_model(eval_batch)[\"logits\"].argmax(dim=1)\n",
    "\n",
    "# Visualize only 5 samples\n",
    "predictions = predictions[:6]\n",
    "\n",
    "label = ['negative', 'positive']\n",
    "for i, prediction in enumerate(predictions[:6]):\n",
    "  sentence = sst2_dataset[\"validation\"][i][\"sentence\"]\n",
    "  correct_label = label[sst2_dataset[\"validation\"][i][\"label\"]]\n",
    "  prediction_label = label[prediction]\n",
    "  print(f\"Sample: {sentence}\")\n",
    "  print(f\"Label: {correct_label}\")\n",
    "  print(f\"Prediction: {prediction_label}\")\n",
    "  print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial showed how to use the composer `Trainer` to fine-tune a pre-trained BERT on a subset of the SST-2 dataset. We focused on the composer's basic functionality, but there are many more tools such as easy to use gradient accumulation and multi-GPU training! Check out many other features at our [documentation](https://docs.mosaicml.com/en/latest)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMWJI/NlM8J5QL/VXsPEoxE",
   "collapsed_sections": [],
   "name": "Composer BERT fine-tuning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
