/* groovylint-disable DuplicateMapLiteral, DuplicateStringLiteral, NestedBlockDepth */

// Cloud settings
pCloud = 'colo-research-01' // name of the jenkins cloud to use

// Git variables. These are populated during the "Prepare" stage
gitUrl = null
gitBranch = null
gitCommit = null
baseBranch = 'dev'  // the branch on which merge-commits should be pushed to dockerhub

// Docker build settings
pytorchDockerBuildMatrix = null // The pytorch docker build matrix
didDockerChange = false
dockerBuildCpuLimit = '4'
dockerBuildMemLimit = '20Gi'
dockerBuildTimeout = '7200'  // timeout for docker builds, in seconds. Builds from scratch can be slow.
// must use the kaniko debug image, as Jenkins needs shell access
// see https://github.com/GoogleContainerTools/kaniko#debug-image
kanikoDockerImage = 'gcr.io/kaniko-project/executor:v1.7.0-debug' // Docker image for kaniko docker builds

// Pytest settings
pytestTimeout = '1800' // timeout to run pytest, in seconds
pytestCpuTestCpuLimit = '4'
pytestCpuTestMemLimit = '20Gi' // memory limit for ram and ephemeral storage
pytestGpuTestNumGpus = 2
pytestGpuTestCpuLimit = '15'
pytestGpuTestMemLimit = '30Gi' // memory limit for ram and ephemeral storage

// Lint settings
lintImage = 'mosaicml/pytorch:latest_cpu'
lintCpuLimit = '2'
lintMemLimit = '7Gi'
pLintTimeout = '1800' // timeout to run lint + doctests, in seconds

// Conda settings
dependenciesChanged = false // Whether meta.yaml or setup.py have changed, (if so, rebuild conda)
condaCpuLimit = '4'
condaMemLimit = '20Gi' // memory limit for ram and ephemeral storage
condaBuildDockerImage = 'continuumio/anaconda-pkg-build:2022.02.09-amd64'  // Docker image for conda builds
condaTimeout = '3600' // timeout for conda builds, in seconds

// Jenkins settings
numDaysOfBuildsToKeep = '7' // number of days to keep builds (so Jenkins doesn't crash)
jenkinsfileRepo = 'https://github.com/mosaicml/testing' // Repo containing helper scripts for Jenkins
jenkinsShellJobName = 'scratch/command2' // The jenkins job name used to spawn sub-jobs
gitCredentialsId = '9cf9add1-2cdd-414b-8160-94bd4ac4a13d' // Jenkins credential ID to use for git clones

// Artifact setting
buildOutputFolder = 'build/output' // Folder where build artifacts are stored
artifactsGlob = "$buildOutputFolder/**" // Files matching this glob will be archived by Jenkins
junitGlob = "$buildOutputFolder/*.junit.xml" // Files matching this glob will be presented Junit test reports
coverageGlob = "$buildOutputFolder/*.coverage.xml" // Files matching this glob will be presented Junit coverage reports

// Spawned builds
builds = [] // List of spawned sub-jobs


properties(
    [
        buildDiscarder(
            logRotator(daysToKeepStr: numDaysOfBuildsToKeep, artifactDaysToKeepStr: numDaysOfBuildsToKeep)
        ),
    ]
)

String cloneJenkinsfilesRepo() {
    // Clone the remote jenkins file in WORKSPACE_TMP
    String jenkinsfileRepoTargetDir = 'jenkinsfiles'
    dir("$WORKSPACE_TMP") {
        checkout([
            $class: 'GitSCM',
            branches: [[name: 'main']],
            doGenerateSubmoduleConfigurations: false,
            extensions: [[$class: 'RelativeTargetDirectory', relativeTargetDir: jenkinsfileRepoTargetDir]],
            submoduleCfg: [],
            changelog: false,
            userRemoteConfigs: [[url: jenkinsfileRepo, credentialsId: gitCredentialsId]]
        ])
    }
    return "$WORKSPACE_TMP/$jenkinsfileRepoTargetDir"
}

void trackBuild(Map buildArgs) {
    // 1. Run a build() command, but manually echo a link to the spawned job, since it may not show up
    //    in blue ocean. See https://issues.jenkins.io/browse/JENKINS-60995.
    // 2. Add the build to the `builds` variable
    buildArgs['propagate'] = false
    def builtJob = build(buildArgs)
    builds << builtJob
    if (builtJob.result == 'SUCCESS') {
        echo "Job ${builtJob.fullDisplayName} was successful. See ${builtJob.absoluteUrl} for details."
    }
    else {
        error "Job ${builtJob.fullDisplayName} failed. See ${builtJob.absoluteUrl} for details."
    }
}

void runLint(String pDockerImage) {
    trackBuild(
        job: jenkinsShellJobName,
        parameters: [
            string(name: 'P_CLOUD', value: pCloud),
            string(name: 'P_GIT_REPO', value: gitUrl),
            string(name: 'P_GIT_COMMIT', value: gitCommit),
            string(name: 'P_EPHEMERAL_STORAGE_LIMIT', value: lintMemLimit),
            string(name: 'P_DOCKER_IMAGE', value: pDockerImage),
            string(name: 'P_TIMEOUT', value: pLintTimeout),
            string(name: 'P_CPU_LIMIT', value: lintCpuLimit),
            string(name: 'P_MEM_LIMIT', value: lintMemLimit), // must include the ephemeral storage limit
            string(name: 'P_COMMAND', value: './.ci/lint_doctests.sh'),
            string(name: 'P_ARTIFACTS_GLOB', value: artifactsGlob),
            string(name: 'P_JUNIT_GLOB', value: junitGlob),
            string(name: 'P_COVERAGE_GLOB', value: coverageGlob),
        ]
    )
}

void scheduleJobForBuildArgs(jobs, String image, buildArgs) {
    // jobs: The list of jobs. Modified in-place.
    // buildArgs: The build args matrix

    String markers = 'not notebooks and not gpu and not vision and not daily'
    Boolean isLintImage = false
    Boolean isVisionImage = false
    Boolean isGpu = false
    def tag = null
    buildArgs.each { key, val ->
        if (key == 'CUDA_VERSION') {
            if (val != 'cpu') {
                markers = 'not notebooks and gpu and not vision and not daily'
                isGpu = true
            }
        }
        if (key == 'TARGET' && val == 'vision_stage') {
            isVisionImage = true
            markers = 'not notebooks and vision'
        }
        if (key == 'TAG') {
            tag = val
            // there could be multiple tags
            isLintImage = isLintImage || tag == lintImage
        }
    }
    String extraDeps = 'all'

    if (isVisionImage) {
        markers = 'not notebooks and vision and not daily'
    }

    jobs << [
        "Pytest - ${tag}" : { -> runPytest(image, markers, extraDeps, isGpu) }
    ]
    if (isLintImage) {
        // and run lint and a dev install on this image
        jobs << [
            'Pytest - extraDeps=dev': { -> runPytest(image, markers, 'dev', isGpu) },
            'Lint': { -> runLint(image) },
        ]
    }
}

void runPytest(String pDockerImage, String markers, String extraDeps, Boolean isGpu) {
    // pDockerImage (str): Base docker image to use.
    // extraDeps (str): The pip extra deps to install -- e.g. "pip install "mosaicml[$extraDeps]".
    // markers (str): Pyetst markers
    // isGpu (Boolean): Whether the test requires gpus
    String nGpus = '0'
    String cpuLimit = pytestCpuTestCpuLimit
    String memLimit = pytestCpuTestMemLimit

    if (isGpu) {
        nGpus = pytestGpuTestNumGpus
        cpuLimit = pytestGpuTestCpuLimit
        memLimit = pytestGpuTestMemLimit
    }

    trackBuild(
        job: jenkinsShellJobName,
        parameters: [
            string(name: 'P_CLOUD', value: pCloud),
            string(name: 'P_GIT_REPO', value: gitUrl),
            string(name: 'P_GIT_COMMIT', value: gitCommit),
            string(name: 'P_DOCKER_IMAGE', value: pDockerImage),
            string(name: 'P_CPU_LIMIT', value: cpuLimit),
            string(name: 'P_MEM_LIMIT', value: memLimit),
            string(name: 'P_TIMEOUT', value: pytestTimeout),
            string(name: 'P_N_GPUS', value: nGpus),
            string(name: 'P_EPHEMERAL_STORAGE_LIMIT', value: memLimit),
            text(name: 'P_COMMAND', value: "./.ci/test.sh '$extraDeps' '$markers'"),
            string(name: 'P_ARTIFACTS_GLOB', value: artifactsGlob),
            string(name: 'P_JUNIT_GLOB', value: junitGlob),
            string(name: 'P_COVERAGE_GLOB', value: coverageGlob),
        ]
    )
}

stage('Prepare') {
    node(pCloud) {
        // Automatically cancel old builds only on PR builds
        // From https://stackoverflow.com/questions/40760716/jenkins-abort-running-build-if-new-one-is-started
        if (env.CHANGE_ID) {  // if it is a PR build
            int buildNumber = env.BUILD_NUMBER as int
            if (buildNumber > 1) {
                milestone(buildNumber - 1)
            }
            milestone(buildNumber)
        }

        def loadedSCM = checkout scm

        gitUrl = loadedSCM.GIT_URL
        gitBranch = loadedSCM.GIT_BRANCH
        gitCommit = loadedSCM.GIT_COMMIT

        if (env.CHANGE_ID) {
            // Use the origin/pr/PR_NUMBER/merge to support commits in external repos
            gitCommit = "origin/pr/${pullRequest.number}/merge"
        }

        echo "gitUrl: $gitUrl"
        echo "gitBranch: $gitBranch"
        echo "gitCommit: $gitCommit"

        def jenkinsfileWorkspace = cloneJenkinsfilesRepo()

        // TODO ravi: Update the testing repo; remove the "Next" before merging
        def getDockerBuildMatrix = load "$jenkinsfileWorkspace/utils/getDockerBuildMatrixNext.groovy"

        isPathModified = load "$jenkinsfileWorkspace/utils/isPathModified.groovy"

        didDockerChange = isPathModified('docker/pytorch/')

        String dockerfile = 'Dockerfile'
        String buildContext = './docker/pytorch'
        String buildMatrix = './docker/pytorch/build_matrix.yaml'
        pytorchDockerBuildMatrix = getDockerBuildMatrix(buildMatrix, buildContext, dockerfile)

        // Keep track of whether dependencies changed, in which case a conda build should be tested
        dependenciesChanged = isPathModified('setup.py') || isPathModified('meta.yaml')
    }
}

stage('Build') {
    def jobs = [:]
    Boolean isMergeCommit = true
    if (env.CHANGE_ID) {
        isMergeCommit = false
    }

    pytorchDockerBuildMatrix.each { entry ->
        // Extract the values from the buildMatrix
        String kanikoCommand = entry[0]  // command is the command to run
        String destinations = entry[1]  // The destinations to append to the kanikoCommand to push the image
        String stagingImage = entry[2]  // stagingImage is where the built docker image is always pushed
        // buildArgs contains the entry from the build matrix. It has the format [{key: key, value: value}, ...].
        def buildArgs = entry[3]

        if (didDockerChange) {
            // If changing docker, build the docker images first
            // Then, run pytest in the newly-built image

            Boolean shouldPushToDestinations = isMergeCommit && gitBranch == baseBranch

            if (shouldPushToDestinations) {
                kanikoCommand = "$kanikoCommand $destinations"
            }

            jobs << [ "$buildArgs": { -> 
                trackBuild(
                    job: jenkinsShellJobName,
                    parameters: [
                        string(name: 'P_CLOUD', value: pCloud),
                        string(name: 'P_GIT_REPO', value: gitUrl),
                        string(name: 'P_GIT_COMMIT', value: gitCommit),
                        string(name: 'P_DOCKER_IMAGE', value: kanikoDockerImage),
                        string(name: 'P_EPHEMERAL_STORAGE_LIMIT', value: dockerBuildMemLimit),
                        text(name: 'P_COMMAND', value: command),
                        string(name: 'P_TIMEOUT', value: dockerBuildTimeout),
                        string(name: 'P_CPU_LIMIT', value: dockerBuildCpuLimit),
                        string(name: 'P_MEM_LIMIT', value: dockerBuildMemLimit),
                        booleanParam(name: 'P_MOUNT_KANIKO_CREDENTIALS', value: true),
                    ]
                )
                def subJobs = [:]
                scheduleJobForBuildArgs(subJobs, stagingImage, buildArgs)
                subJobs.failFast = true
                parallel(subJobs)
            }]
        }
        else {
            // If not rebuilding the docker image, then run all tests on the existing images
            String existingImageTag = buildArgs['TAGS'][0]
            scheduleJobForBuildArgs(jobs, existingImageTag, buildArgs)
        }
    }

    if (dependenciesChanged) {
        // regardless of whether the docker image changed, rebuild the conda package
        // if the dependencies changed
        jobs << [
            'Conda': { ->
                trackBuild(
                    job: jenkinsShellJobName,
                    parameters: [
                        string(name: 'P_CLOUD', value: pCloud),
                        string(name: 'P_GIT_REPO', value: gitUrl),
                        string(name: 'P_GIT_COMMIT', value: gitCommit),
                        string(name: 'P_EPHEMERAL_STORAGE_LIMIT', value: condaMemLimit),
                        string(name: 'P_DOCKER_IMAGE', value: condaBuildDockerImage),
                        string(name: 'P_TIMEOUT', value: condaTimeout), // Conda builds take longer
                        string(name: 'P_CPU_LIMIT', value: condaCpuLimit),
                        string(name: 'P_MEM_LIMIT', value: condaMemLimit),  // must include the ephemeral storage limit
                        string(name: 'P_COMMAND', value: './.ci/build_conda.sh')
                    ]
                )
            }
        ]
    }
    jobs.failFast = true
    try {
        parallel(jobs)
    }
    finally {
        stage('Merge Artifacts') {
            node(pCloud) {
                checkout scm  // checking out the SCM so the coverage report can load the source
                builds.each { item ->
                    copyArtifacts(
                        projectName: item.fullProjectName,
                        selector: specific("${item.number}"),
                        fingerprintArtifacts: true,
                        optional: true,
                    )
                }

                sh "mkdir -p $buildOutputFolder"

                archiveArtifacts(artifacts: artifactsGlob, fingerprint: true, allowEmptyArchive: true)
                junit(allowEmptyResults: true, testResults: junitGlob, checksName: 'Tests')
                publishCoverage(
                    adapters: [cobertura(path: coverageGlob, mergeToOneReport: true)],
                    calculateDiffForChangeRequests: true,
                    sourceFileResolver: [level: 'STORE_LAST_BUILD']
                )
            }
        }
    }
}
