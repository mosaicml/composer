/* groovylint-disable DuplicateMapLiteral, DuplicateStringLiteral, NestedBlockDepth */
pCloud = 'colo-research-01'
gitUrl = null
gitBranch = null
gitCommit = null
pTimeoutDocker = '3600'
pTimeout = '1800' // in seconds
dependenciesChanged = null
pytorchDockerBuildMatrix = null
isPathModified = null
builds = []
jenkinsShellJobName = 'scratch/command2'
numDaysOfBuildsToKeep = '7'
jenkinsfileRepo = 'https://github.com/mosaicml/testing'
gitCredentialsId = '9cf9add1-2cdd-414b-8160-94bd4ac4a13d'
buildOutputFolder = 'build/output'
artifactsGlob = "$buildOutputFolder/**"
junitGlob = "$buildOutputFolder/*.junit.xml"
coverageGlob = "$buildOutputFolder/*.coverage.xml"
condaBuildDockerImage = 'continuumio/anaconda-pkg-build:2022.02.09-amd64'
// must use the kaniko debug image, as Jenkins needs shell access
// see https://github.com/GoogleContainerTools/kaniko#debug-image
kanikoDockerImage = 'gcr.io/kaniko-project/executor:v1.7.0-debug'

properties(
    [
        buildDiscarder(
            logRotator(daysToKeepStr: numDaysOfBuildsToKeep, artifactDaysToKeepStr: numDaysOfBuildsToKeep)
        ),
    ]
)

String cloneJenkinsfilesRepo() {
    // Clone the remote jenkins file in WORKSPACE_TMP
    String jenkinsfileRepoTargetDir = 'jenkinsfiles'
    dir("$WORKSPACE_TMP") {
        checkout([
            $class: 'GitSCM',
            branches: [[name: 'main']],
            doGenerateSubmoduleConfigurations: false,
            extensions: [[$class: 'RelativeTargetDirectory', relativeTargetDir: jenkinsfileRepoTargetDir]],
            submoduleCfg: [],
            changelog: false,
            userRemoteConfigs: [[url: jenkinsfileRepo, credentialsId: gitCredentialsId]]
        ])
    }
    return "$WORKSPACE_TMP/$jenkinsfileRepoTargetDir"
}

void trackBuild(Map buildArgs) {
    // 1. Run a build() command, but manually echo a link to the spawned job, since it may not show up
    //    in blue ocean. See https://issues.jenkins.io/browse/JENKINS-60995.
    // 2. Add the build to the `builds` variable
    buildArgs['propagate'] = false
    def builtJob = build(buildArgs)
    builds << builtJob
    if (builtJob.result == 'SUCCESS') {
        echo "Job ${builtJob.fullDisplayName} was successful. See ${builtJob.absoluteUrl} for details."
    }
    else {
        error "Job ${builtJob.fullDisplayName} failed. See ${builtJob.absoluteUrl} for details."
    }
}

String getDockerImageName(String baseImage, String pythonVersion, Boolean gpu) {
    String pytorchVersion = pythonVersion == '3.9' ? '1.10.0' : '1.9.1'
    String cudaVersion = 'cpu'
    if (gpu) {
        cudaVersion = pythonVersion == '3.9' ? 'cu113' : 'cu111'

    }
    return "${baseImage}:${pytorchVersion}_${cudaVersion}-python${pythonVersion}-ubuntu20.04"
}

lintImage = getDockerImageName('mosaicml/pytorch', '3.9', false)
visionImage = getDockerImageName('mosaicml/pytorch_vision', '3.9', true)

void runLint(String pDockerImage) {
    trackBuild(
        job: jenkinsShellJobName,
        parameters: [
            string(name: 'P_CLOUD', value: pCloud),
            string(name: 'P_GIT_REPO', value: gitUrl),
            string(name: 'P_GIT_COMMIT', value: gitCommit),
            string(name: 'P_EPHEMERAL_STORAGE_LIMIT', value: '7Gi'),
            string(name: 'P_DOCKER_IMAGE', value: pDockerImage),
            string(name: 'P_TIMEOUT', value: pTimeout),
            string(name: 'P_CPU_LIMIT', value: '2'),
            string(name: 'P_MEM_LIMIT', value: '7Gi'), // must include the ephemeral storage limit
            string(name: 'P_COMMAND', value: './.ci/lint_doctests.sh'),
            string(name: 'P_ARTIFACTS_GLOB', value: artifactsGlob),
            string(name: 'P_JUNIT_GLOB', value: junitGlob),
            string(name: 'P_COVERAGE_GLOB', value: coverageGlob),
        ]
    )
}

void runPytest(String pDockerImage, String markers, String extraDeps, Boolean isGpu) {
    // pDockerImage (str): Base docker image to use.
    // extraDeps (str): The pip extra deps to install -- e.g. pip install mosaicml[$extraDeps].
    // markers (str): Pyetst markers
    // isGpu (Boolean): Whether the test requires gpus
    String nGpus = '0'
    String cpuLimit = '2'

    if (isGpu) {
        nGpus = '2'
        cpuLimit = '16' // 8 cpu per gpu
    }

    trackBuild(
        job: jenkinsShellJobName,
        parameters: [
            string(name: 'P_CLOUD', value: pCloud),
            string(name: 'P_GIT_REPO', value: gitUrl),
            string(name: 'P_GIT_COMMIT', value: gitCommit),
            string(name: 'P_DOCKER_IMAGE', value: pDockerImage),
            string(name: 'P_CPU_LIMIT', value: cpuLimit),
            string(name: 'P_MEM_LIMIT', value: '30Gi'),  // must include the ephemeral storage limit
            string(name: 'P_TIMEOUT', value: pTimeout),
            string(name: 'P_N_GPUS', value: nGpus),
            string(name: 'P_EPHEMERAL_STORAGE_LIMIT', value: '30Gi'),
            text(name: 'P_COMMAND', value: "./.ci/test.sh '$extraDeps' '$markers'"),
            string(name: 'P_ARTIFACTS_GLOB', value: artifactsGlob),
            string(name: 'P_JUNIT_GLOB', value: junitGlob),
            string(name: 'P_COVERAGE_GLOB', value: coverageGlob),
        ]
    )
}

stage('Prepare') {
    node(pCloud) {
        // Automatically cancel old builds only on PR builds
        // From https://stackoverflow.com/questions/40760716/jenkins-abort-running-build-if-new-one-is-started
        if (env.CHANGE_ID) {  // if it is a PR build
            int buildNumber = env.BUILD_NUMBER as int
            if (buildNumber > 1) {
                milestone(buildNumber - 1)
            }
            milestone(buildNumber)
        }

        def loadedSCM = checkout scm

        gitUrl = loadedSCM.GIT_URL
        gitBranch = loadedSCM.GIT_BRANCH
        gitCommit = loadedSCM.GIT_COMMIT

        if (env.CHANGE_ID) {
            // Use the origin/pr/PR_NUMBER/merge to support commits in external repos
            gitCommit = "origin/pr/${pullRequest.number}/merge"
        }

        echo "gitUrl: $gitUrl"
        echo "gitBranch: $gitBranch"
        echo "gitCommit: $gitCommit"

        def jenkinsfileWorkspace = cloneJenkinsfilesRepo()

        def getDockerBuildMatrix = load "$jenkinsfileWorkspace/utils/getDockerBuildMatrix.groovy"

        isPathModified = load "$jenkinsfileWorkspace/utils/isPathModified.groovy"

        if (isPathModified('docker/pytorch/')) {
            Boolean shouldPush = gitBranch == 'dev'
            String dockerfile = 'Dockerfile'
            String buildContext = './docker/pytorch'
            String buildMatrix = './docker/pytorch/build_matrix.yaml'
            pytorchDockerBuildMatrix = getDockerBuildMatrix(buildMatrix, buildContext, dockerfile, shouldPush)
        }
        // Keep track of whether dependencies changed, in which case a conda build should be tested
        dependenciesChanged = isPathModified('setup.py') || isPathModified('meta.yaml')
    }
}

stage('Build') {
    def jobs = [:]
    Boolean isMergeCommit = true
    if (env.CHANGE_ID) {
        isMergeCommit = false
    }
    if (pytorchDockerBuildMatrix && (!isMergeCommit || gitBranch == 'dev')) {
        // If changing docker, build the docker images first
        // Then, run pytest in the newly-built image
        // Only need to run the build if it's a PR commit (not a merge commit), or it's a merge commit into dev.
        pytorchDockerBuildMatrix.each { entry ->
            String command = entry[0]  // command is the command to run
            String stagingImage = entry[1]  // stagingImage is where the built docker image is pushed
            // buildArgs contains the entry from the build matrix. It has the format [{key: key, value: value}, ...].
            def buildArgs = entry[2]
            jobs << [ "$buildArgs": { -> 
                trackBuild(
                    job: jenkinsShellJobName,
                    parameters: [
                        string(name: 'P_CLOUD', value: pCloud),
                        string(name: 'P_GIT_REPO', value: gitUrl),
                        string(name: 'P_GIT_COMMIT', value: gitCommit),
                        string(name: 'P_DOCKER_IMAGE', value: kanikoDockerImage),
                        string(name: 'P_EPHEMERAL_STORAGE_LIMIT', value: '30Gi'),
                        text(name: 'P_COMMAND', value: command),
                        string(name: 'P_TIMEOUT', value: pTimeoutDocker),
                        string(name: 'P_CPU_LIMIT', value: '4'),
                        string(name: 'P_MEM_LIMIT', value: '30Gi'),  // must include the ephemeral storage limit
                        booleanParam(name: 'P_MOUNT_KANIKO_CREDENTIALS', value: true),
                    ]
                )
                if (isMergeCommit) {
                    // no need to run tests again
                    return
                }
                String markers = 'not notebooks and not gpu and not vision and not daily'
                Boolean isLintImage = false
                Boolean isVisionImage = false
                Boolean isGpuImage = false
                def tag = null
                buildArgs.each { key, val ->
                    if (key == 'CUDA_VERSION') {
                        if (val != 'cpu') {
                            markers = 'not notebooks and gpu and not vision and not daily'
                            isGpuImage = true
                        }
                    }
                    if (key == 'TAG') {
                        tag = val
                        // there could be multiple tags
                        isLintImage = isLintImage || tag == lintImage
                        isVisionImage = isVisionImage || tag == visionImage
                    }
                }
                String extraDeps = 'all'

                if (isVisionImage) {
                    markers = 'not notebooks and vision and not daily'
                }

                def subJobs = [
                    "Pytest - ${tag}" : { -> runPytest(stagingImage, markers, extraDeps, isGpuImage) }
                ]
                if (isLintImage) {
                    // and run lint and a dev install on this image
                    subJobs << [
                        'Pytest - extraDeps=dev': { -> runPytest(stagingImage, markers, 'dev', false) },
                        'Lint': { -> runLint(stagingImage) },
                    ]
                }
                subJobs.failFast = true
                parallel(subJobs)
            }]
        }
    }
    else if (!isMergeCommit) {
        // if not rebuilding the docker image, but it's not a merge commit,
        // just run these checks on the latest images. No need to re-run the
        // tests on merge commits, as the PR must have passed these checks already
        // to have been merged.
        jobs << [
            'Python 3.7': { ->
                runPytest(
                    getDockerImageName('mosaicml/pytorch', '3.7', false),
                    'not notebooks and not gpu and not vision and not daily', // markers
                    'all', // extraDeps
                    false, // gpu
                )
            },
            'Python 3.8': { ->
                runPytest(
                    getDockerImageName('mosaicml/pytorch', '3.8', false),
                    'not notebooks and not gpu and not vision and not daily', // markers
                    'all', // extraDeps
                    false, // gpu
                )
            },
            'Python 3.9': { ->
                runPytest(
                    getDockerImageName('mosaicml/pytorch', '3.9', false),
                    'not notebooks and not gpu and not vision and not daily', // markers
                    'all', // extraDeps
                    false, // gpu
                )
            },
            'Python 3.7 - GPU': { ->
                runPytest(
                    getDockerImageName('mosaicml/pytorch', '3.7', true),
                    'not notebooks and gpu and not vision and not daily', // markers
                    'all', // extraDeps
                    true, // gpu
                )
            },
            'Python 3.7 - Vision': { ->
                runPytest(
                    visionImage,
                    'not notebooks and vision and not daily',
                    'all', // extraDeps
                    true, // gpu
                )
            },
            'Lint': { -> runLint(lintImage) },
            'Python 3.7 - extraDeps=dev': { ->
                runPytest(
                    getDockerImageName('mosaicml/pytorch', '3.7', false),
                    'not notebooks and not gpu and not vision and not daily', // markers
                    'dev', // extraDeps
                    false, // gpu
                )
            },
        ]
    }

    if (!isMergeCommit && dependenciesChanged) {
        // regardless of whether the docker image changed, rebuild the conda package
        // if the dependencies changed
        jobs << [
            'Conda': { ->
                trackBuild(
                    job: jenkinsShellJobName,
                    parameters: [
                        string(name: 'P_CLOUD', value: pCloud),
                        string(name: 'P_GIT_REPO', value: gitUrl),
                        string(name: 'P_GIT_COMMIT', value: gitCommit),
                        string(name: 'P_EPHEMERAL_STORAGE_LIMIT', value: '30Gi'),
                        string(name: 'P_DOCKER_IMAGE', value: condaBuildDockerImage),
                        string(name: 'P_TIMEOUT', value: '3600'), // Conda builds take longer
                        string(name: 'P_CPU_LIMIT', value: '4'),
                        string(name: 'P_MEM_LIMIT', value: '30Gi'),  // must include the ephemeral storage limit
                        string(name: 'P_COMMAND', value: './.ci/build_conda.sh')
                    ]
                )
            }
        ]
    }
    jobs.failFast = true
    try {
        parallel(jobs)
    }
    finally {
        stage('Merge Artifacts') {
            node(pCloud) {
                checkout scm  // checking out the SCM so the coverage report can load the source
                builds.each { item ->
                    copyArtifacts(
                        projectName: item.fullProjectName,
                        selector: specific("${item.number}"),
                        fingerprintArtifacts: true,
                        optional: true,
                    )
                }

                sh "mkdir -p $buildOutputFolder"

                archiveArtifacts(artifacts: artifactsGlob, fingerprint: true, allowEmptyArchive: true)
                junit(allowEmptyResults: true, testResults: junitGlob, checksName: 'Tests')
                publishCoverage(
                    adapters: [cobertura(path: coverageGlob, mergeToOneReport: true)],
                    calculateDiffForChangeRequests: true,
                    sourceFileResolver: [level: 'STORE_LAST_BUILD']
                )
            }
        }
    }
}
